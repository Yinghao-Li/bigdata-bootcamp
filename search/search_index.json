{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"001-python/","text":"Python Tools for Data Analysis lecturer: Yinghao Li 1. Python Installation: Anaconda Info If you don't feel like using the local environment , you can try Google Colab for a free online python environment. The examples are also available on Colab: Ignore this session if you already have a python environment. Anaconda is a complete, open source data science package with a community of over 6 million users. It is easy to download and install; and it supports Linux, macOS, and Windows ( source ). In this tutorial, we'll use Miniconda for minimal installation. Please refer to this page for the difference between Anaconda and Miniconda and which one to choose. 1.1. Windows and macOS Download the latest Miniconda installer from the official website . Install the package according to the instructions. Start to use conda environment with Anaconda Prompt or other shells if you enabled this feature during installation. Warning Notice: To use conda command in other shells/prompts, you need to add the conda directory to your PATH environment variable. Info Please refer to this page for more information about Anaconda installation on Windows and this page on MacOS. 1.2. Linux with terminal Start the terminal. Switch to ~/Download/ with command cd ~/Download/ . If the path does not exist, create one using mkdir ~/Download/ . Download the latest Linux Miniconda distribution using wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh . Once the download is complete, install Miniconda using bash Miniconda3-latest-Linux-x86_64.sh . Follow the prompts on the installer screens. If you are unsure about any setting, accept the defaults. You can change them later. To make the changes take effect, close and then re-open your terminal window or use the command source ~/.bashrc . If you are using zsh or other shells, make sure conda is initiated. To do this, switch back to bash and type the command conda init <shell name> . Info Please refer to this page for more information about Anaconda installation on Linux. 1.3. Verify your installation You can use the command conda list to check your conda installation. If the terminal returns a bunch of python packages, then your installation is successful. Info Please refer to this page for more information. 1.4. Conda environment With conda, you can easily create, remove, and update environments, each with an independent version of Python interpreter and Python packages. This is always desirable when you work on different Python projects with different (often conflicting) package dependencies. In this tutorial, we will use the default base environment. For more information on environment management, please refer to conda: managing environments . 2. Package Installation If you are using Anaconda or Miniconda, you can use the Anaconda package manager conda . You can also use other managers such as pip when the packages are not provided by any conda channels. To look for a specific package, you can visit this website and type the name of that package in the search box. For today's instruction, we need to install numpy , matplotlib , scikit-learn and pandas . First, switch to your conda environment using conda activate <env name> (not necessary if you are using the default base environment), then install those packages using the following commands: conda install -c conda-forge numpy matplotlib scikit-learn pandas The package manager will automatically install the dependencies. If you install scikit-learn first, which depends on numpy , you don't have to install numpy manually and the conda package solver will do it for you. If you prefer a fancier and more powerful python shell, you can choose to install ipython and jupyter notebook . conda install -c conda-forge ipython conda install jupyter Jupyter notebook allows you to run your commands using the browser as an interface instead of the terminal. 3. Basic Python Concepts Info A more comprehensive tutorial can be found on the Stanford CS231n website . We use Python >= 3.9 in this tutorial. Warning Notice that previous Python interpreter versions may behave differently. Please refer to the official document for more details. First, in your terminal, type python or ipython or jupyter notebook to start an interactive python shell. ipython or jupyter notebook is recommended. Info The tutorial is also on Google Colab: 3.1. Variable definition, input and output (print) We do not need to specify the variable type while defining a variable. The interpreter will automatically infer the data type from the assigned value. a = 123 b = '123' c = \"1234\" print(a, b, c, type(a), type(b), type(c)) A variable can be overwritten by a different type: a = 123.456 print(type(a)) a = '123' print(type(a)) The input method allows you to interactively input information into the program through CLI: x = input('Input something: ') print(x, type(x)) Info Input is rarely used unless you are developing CLI programs. A more practical input method is argparse . 3.2. List, tuple, set and dictionary List is a collection that is ordered and changeable . It allows duplicate members. Tuple is a collection that is ordered but not changeable . It also allows duplicate members. Set is a collection that is unordered and unindexed . It does not allow duplicate members. Elements in a set cannot be retrieved by index. Dictionary is a collection that is ordered , changeable and indexed . It does not allow duplicate members. Warning Notice that Dictionary used to be unordered before Python 3.7 . _list = [1, 2, 1.2, '1', '2', 1] # this is a list _tuple = (1, 2, 1.2, '1', '2', 1) # this is a tuple _set = {1, 2, 1.2, '1', '2', 1} # this is a set _dict = { # this is a dict 1: '111', 2: '222', '1': 567, 2.2: ['J', 'Q', 'K'] } print(_list, '\\n', _tuple, '\\n', _set, '\\n', _dict) Access elements print(_list[0], _list[-2], _list[1: 3]) print(_tuple[1], _tuple[-2]) print(_set[0], _set[-1]) print(_dict[1], _dict['1'], _dict[2.2]) Shallow copy a = _list a[0] = 888 print(a, '\\n', _list) 3.3. If else if 888 not in _dict.keys(): _dict[888] = '???' elif 999 not in _dict.keys(): _dict[999] = '!@#$%' else: _dict['qwert'] = 'poiuy' 3.4. Loops Info Note: in Python, the indent is used to define a scope instead of curly brackets {} . Usually, people use 4 whitespaces or one tab character \\t as one layer of indent. Be sure to make it consistent throughout the file. for loop: for x in _list: print(x) for i in range(len(_list)): print(_list[i]) while loop: i = 0 while i != len(_list): print(_list[i]) i += 1 3.5 Function Define a function: def my_func(x): x += 1 print('in function: ', x) return x Call a function t = 10 tt = my_func(t) print(f'out of funciton, t: {t}, tt: {tt}') 4. Basic Numpy Usage 4.1. Array creation A numpy array is a grid of values, all of the same type, and is indexed by a tuple of integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension. We can initialize numpy arrays from nested Python lists, and access elements using square brackets: import numpy as np a = np.array([1, 2, 3]) # Create a rank 1 array print(type(a), a.dtype) print(a.shape) print(a[1]) b = np.array([[1,2,3],[4,5,6]]) # Create a rank 2 array print(b.shape) print(b[0, 0], b[0, 1], b[1, 0]) Change the type of an array: print(a.dtype) a = a.astype(float) print(a.dtype) Other array creation methods: a = np.zeros((2,2)) # Create an array of all zeros print(a) b = np.ones((1,2)) # Create an array of all ones print(b) c = np.full((2,2), 7, dtype=np.float32) # Create a constant array print(c) d = np.eye(3) # Create a 3x3 identity matrix print(d) e = np.random.random((3,3)) # Create an array filled with random values print(e) 4.2. Array indexing Similar to Python lists, numpy arrays can be sliced. # Create a rank 1 array and reshape it to a 3x4 matrix a = np.arange(12).reshape(3, 4) b = a[:2, 1:3] print(a) print(b) # Shallow copy b[0, 0] = 888 print(a) You can mix integer indexing with slice indexing. However, integer indexing will yield an array of lower rank than the original array: row_r1 = a[1, :] # Rank 1 view of the second row of a row_r2 = a[1:2, :] # Rank 2 view of the second row of a print(row_r1, row_r1.shape) print(row_r2, row_r2.shape) You can also access array elements through lists: x = [0, 1, 2] y = [3, 1, 0] print(a[x, y]) Or through a boolean array: b = a > 4 print(b) print(a[b]) 4.3. Array math Basic mathematical functions operate element-wise on arrays, and are available both as operator overloads and as functions in the numpy module: x = np.arange(1, 5, dtype=float).reshape(2, 2) y = np.arange(5, 9, dtype=float).reshape(2, 2) print(x) print(y) # Elementwise sum print(x + y) print(np.add(x, y)) # Elementwise difference print(x - y) print(np.subtract(x, y)) # Elementwise product print(x * y) print(np.multiply(x, y)) # Elementwise division print(x / y) print(np.divide(x, y)) # Elementwise square print(x ** 2) print(np.power(x, 2)) # Elementwise square root print(x ** 0.5) print(np.sqrt(x)) Matrix multiplication is realized by np.dot or operator @ : x = np.arange(1, 5, dtype=float).reshape(2, 2) y = np.arange(5, 9, dtype=float).reshape(2, 2) print(x) print(y) v = np.array([9, 10], dtype=float) w = np.array([11, 12], dtype=float) # Inner product print(v.dot(w)) print(np.dot(v, w)) print(v @ w) # Matrix / vector product print(x.dot(v)) print(np.dot(x, v)) print(x @ v) # Matrix / matrix product print(x.dot(y)) print(np.dot(x, y)) print(x @ y) {{ hint_warning }} Attention: np.dot() and @ behaves differently when the matrix rank is larger than 2. {{ _hint }} Numpy also provides functions for performing computations within an array: print(np.sum(x)) # Compute sum of all elements; prints \"10\" print(x.sum()) # same as above print(np.sum(x, axis=0)) # Compute sum of each column; prints \"[4 6]\" print(np.sum(x, axis=1)) # Compute sum of each row; prints \"[3 7]\" To transpose a matrix, use the T attribute of an array object: print(x.T) If you have a rank >2 matrix, you can use np.transpose to specify how to permute the axes: x = np.arange(24).reshape(2, 3, 4) print(x.transpose(1, 0, 2).shape) 5. Using Matplotlib for Visualization import numpy as np import matplotlib.pyplot as plt # %matplotlib qt # Compute the x and y coordinates for points on a sine curve x = np.arange(0, 3 * np.pi, 0.1) y = np.sin(x) # Plot the points using matplotlib plt.plot(x, y) plt.show() # You must call plt.show() to make graphics appear. Info Note: for jupyter notebook, you can use the command %matplotlib inline to make the graphics embedded in the editor or %matplotlib qt to make them pop out. To plot multiple lines at once, and add a title, legend, and axis labels: x = np.arange(0, 3 * np.pi, 0.1) y_sin = np.sin(x) y_cos = np.cos(x) # Plot the points using matplotlib plt.plot(x, y_sin) plt.plot(x, y_cos) plt.xlabel('x axis label') plt.ylabel('y axis label') plt.title('Sine and Cosine') plt.legend(['Sine', 'Cosine']) plt.show() You can plot different things in the same figure using the subplot function. Here is an example: # Set up a subplot grid that has height 2 and width 1, # and set the first such subplot as active. plt.subplot(2, 1, 1) # Make the first plot plt.plot(x, y_sin) plt.title('Sine') # Set the second subplot as active, and make the second plot. plt.subplot(2, 1, 2) plt.plot(x, y_cos) plt.title('Cosine') # Show the figure. plt.show() 6. Pandas and Scikit-Learn for Data Science In this section, we will look at a data science example using pandas as data management tool and scikit-learn (sklearn) as algorithm implementation. This section is modified from this tutorial . 6.1. Import packages import numpy as np import pandas as pd # automatically split the data into training and test set from sklearn.model_selection import train_test_split from sklearn import preprocessing # classifiers and regressors from sklearn.ensemble import RandomForestRegressor # Construct a Pipeline from the given estimators from sklearn.pipeline import make_pipeline # Exhaustive search over specified parameter values for an estimator. from sklearn.model_selection import GridSearchCV # Training objective and evaluation metrics from sklearn.metrics import mean_squared_error, r2_score # For model persistence # you can use `from sklearn.externals import joblib` if your sklearn version is earlier than 0.23 import joblib 6.2. Load data You can download the data by clicking the link or using wget : wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv and move the file to your current folder. Then, load the csv data into memory through pandas : data = pd.read_csv('winequality-red.csv', sep=';') Or, you can directly load the data through URL. dataset_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv' data = pd.read_csv(dataset_url, sep=';') You can also load datasets stored in other formats with pandas . A detailed document is at pandas: io . 6.3. Take a look of the loaded data The data loaded is stored in the type of pandas.core.frame.DataFrame To give a peak of the data, we can use print(data) This will return a nice-looking preview of the elements in the DataFrame. To view the name of the features of a DataFrame, one can use print(data.keys()) To access one column, i.e., all instances of a feature, e.g., pH , one can use # These will return the same result print(data['pH']) print(data.pH) To access a row, you need the DataFrame.iloc attribute: print(data.iloc[10]) We can also easily print some summary statistics: print(data.describe()) 6.4. Split data First, let's separate our target (y) feature from our input (X) features and divide the dataset into training and test sets using the train_test_split function: y = data.quality X = data.drop('quality', axis=1) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=0, stratify=y ) Stratifying your sample by the target variable will ensure your training set looks similar to your test set, making your evaluation metrics more reliable. 6.5. Pre-processing Standardization is the process of subtracting the means from each feature and then dividing by the feature standard deviations. It is a common requirement for machine learning tasks. Many algorithms assume that all features are centered around zero and have approximately the same variance. scaler = preprocessing.StandardScaler().fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # To prove the trainig and testing sets have (nearly) zero mean and one deviation print(X_train_scaled.mean(axis=0)) print(X_train_scaled.std(axis=0)) print(X_test_scaled.mean(axis=0)) print(X_test_scaled.std(axis=0)) 6.6. Fit the model If we do not need to fine-tune the hyperparameters, we can define a random forest regression model with the default hyperparameters and fit the model using regr = RandomForestRegressor() regr.fit(X_train_scaled, y_train) To examine the performance, we use the test set to calculate the scores pred = regr.predict(X_test_scaled) print(r2_score(y_test, pred)) print(mean_squared_error(y_test, pred)) 6.7. Define the cross-validation pipeline Fine-tuning hyperparameters is an important job in Machine Learning since a set of carefully chosen hyperparameters may greatly improve the performance of the model. In practice, when we set up the cross-validation pipeline, we won't even need to manually fit the data. Instead, we'll simply declare the class object, like so: pipeline = make_pipeline( preprocessing.StandardScaler(), RandomForestRegressor(n_estimators=100) ) To check the hyperparameters, we may use print pipeline.get_params() or refer to the official document . Now, let's declare the hyperparameters we want to tune through cross-validation. hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'], 'randomforestregressor__max_depth': [None, 5, 3, 1] } Then, we can set a 10-fold cross validation as simple as clf = GridSearchCV(pipeline, hyperparameters, cv=10) Finally, we can automatically fine-tune the model using clf.fit(X_train, y_train) After the model fitting, if we want to check the best hyperparameters, we can use print(clf.best_params_) Same as before, we evaluate the fitted model on test set pred = clf.predict(X_test) print(r2_score(y_test, pred)) print(mean_squared_error(y_test, pred)) 6.8. Save and load models After training, we may want to save the trained model for future use. For this purpose, we can use joblib.dump(clf, 'rf_regressor.pkl') When you want to load the model again, simply use this function: clf2 = joblib.load('rf_regressor.pkl') # Predict data set using loaded model clf2.predict(X_test) Info A more comprehensive example of scikit-learn can be found here .","title":"Python for Data Analysis"},{"location":"001-python/#python-tools-for-data-analysis","text":"lecturer: Yinghao Li","title":"Python Tools for Data Analysis"},{"location":"001-python/#1-python-installation-anaconda","text":"Info If you don't feel like using the local environment , you can try Google Colab for a free online python environment. The examples are also available on Colab: Ignore this session if you already have a python environment. Anaconda is a complete, open source data science package with a community of over 6 million users. It is easy to download and install; and it supports Linux, macOS, and Windows ( source ). In this tutorial, we'll use Miniconda for minimal installation. Please refer to this page for the difference between Anaconda and Miniconda and which one to choose.","title":"1. Python Installation: Anaconda"},{"location":"001-python/#11-windows-and-macos","text":"Download the latest Miniconda installer from the official website . Install the package according to the instructions. Start to use conda environment with Anaconda Prompt or other shells if you enabled this feature during installation. Warning Notice: To use conda command in other shells/prompts, you need to add the conda directory to your PATH environment variable. Info Please refer to this page for more information about Anaconda installation on Windows and this page on MacOS.","title":"1.1. Windows and macOS"},{"location":"001-python/#12-linux-with-terminal","text":"Start the terminal. Switch to ~/Download/ with command cd ~/Download/ . If the path does not exist, create one using mkdir ~/Download/ . Download the latest Linux Miniconda distribution using wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh . Once the download is complete, install Miniconda using bash Miniconda3-latest-Linux-x86_64.sh . Follow the prompts on the installer screens. If you are unsure about any setting, accept the defaults. You can change them later. To make the changes take effect, close and then re-open your terminal window or use the command source ~/.bashrc . If you are using zsh or other shells, make sure conda is initiated. To do this, switch back to bash and type the command conda init <shell name> . Info Please refer to this page for more information about Anaconda installation on Linux.","title":"1.2. Linux with terminal"},{"location":"001-python/#13-verify-your-installation","text":"You can use the command conda list to check your conda installation. If the terminal returns a bunch of python packages, then your installation is successful. Info Please refer to this page for more information.","title":"1.3. Verify your installation"},{"location":"001-python/#14-conda-environment","text":"With conda, you can easily create, remove, and update environments, each with an independent version of Python interpreter and Python packages. This is always desirable when you work on different Python projects with different (often conflicting) package dependencies. In this tutorial, we will use the default base environment. For more information on environment management, please refer to conda: managing environments .","title":"1.4. Conda environment"},{"location":"001-python/#2-package-installation","text":"If you are using Anaconda or Miniconda, you can use the Anaconda package manager conda . You can also use other managers such as pip when the packages are not provided by any conda channels. To look for a specific package, you can visit this website and type the name of that package in the search box. For today's instruction, we need to install numpy , matplotlib , scikit-learn and pandas . First, switch to your conda environment using conda activate <env name> (not necessary if you are using the default base environment), then install those packages using the following commands: conda install -c conda-forge numpy matplotlib scikit-learn pandas The package manager will automatically install the dependencies. If you install scikit-learn first, which depends on numpy , you don't have to install numpy manually and the conda package solver will do it for you. If you prefer a fancier and more powerful python shell, you can choose to install ipython and jupyter notebook . conda install -c conda-forge ipython conda install jupyter Jupyter notebook allows you to run your commands using the browser as an interface instead of the terminal.","title":"2. Package Installation"},{"location":"001-python/#3-basic-python-concepts","text":"Info A more comprehensive tutorial can be found on the Stanford CS231n website . We use Python >= 3.9 in this tutorial. Warning Notice that previous Python interpreter versions may behave differently. Please refer to the official document for more details. First, in your terminal, type python or ipython or jupyter notebook to start an interactive python shell. ipython or jupyter notebook is recommended. Info The tutorial is also on Google Colab:","title":"3. Basic Python Concepts"},{"location":"001-python/#31-variable-definition-input-and-output-print","text":"We do not need to specify the variable type while defining a variable. The interpreter will automatically infer the data type from the assigned value. a = 123 b = '123' c = \"1234\" print(a, b, c, type(a), type(b), type(c)) A variable can be overwritten by a different type: a = 123.456 print(type(a)) a = '123' print(type(a)) The input method allows you to interactively input information into the program through CLI: x = input('Input something: ') print(x, type(x)) Info Input is rarely used unless you are developing CLI programs. A more practical input method is argparse .","title":"3.1. Variable definition, input and output (print)"},{"location":"001-python/#32-list-tuple-set-and-dictionary","text":"List is a collection that is ordered and changeable . It allows duplicate members. Tuple is a collection that is ordered but not changeable . It also allows duplicate members. Set is a collection that is unordered and unindexed . It does not allow duplicate members. Elements in a set cannot be retrieved by index. Dictionary is a collection that is ordered , changeable and indexed . It does not allow duplicate members. Warning Notice that Dictionary used to be unordered before Python 3.7 . _list = [1, 2, 1.2, '1', '2', 1] # this is a list _tuple = (1, 2, 1.2, '1', '2', 1) # this is a tuple _set = {1, 2, 1.2, '1', '2', 1} # this is a set _dict = { # this is a dict 1: '111', 2: '222', '1': 567, 2.2: ['J', 'Q', 'K'] } print(_list, '\\n', _tuple, '\\n', _set, '\\n', _dict) Access elements print(_list[0], _list[-2], _list[1: 3]) print(_tuple[1], _tuple[-2]) print(_set[0], _set[-1]) print(_dict[1], _dict['1'], _dict[2.2]) Shallow copy a = _list a[0] = 888 print(a, '\\n', _list)","title":"3.2. List, tuple, set and dictionary"},{"location":"001-python/#33-if-else","text":"if 888 not in _dict.keys(): _dict[888] = '???' elif 999 not in _dict.keys(): _dict[999] = '!@#$%' else: _dict['qwert'] = 'poiuy'","title":"3.3. If else"},{"location":"001-python/#34-loops","text":"Info Note: in Python, the indent is used to define a scope instead of curly brackets {} . Usually, people use 4 whitespaces or one tab character \\t as one layer of indent. Be sure to make it consistent throughout the file. for loop: for x in _list: print(x) for i in range(len(_list)): print(_list[i]) while loop: i = 0 while i != len(_list): print(_list[i]) i += 1","title":"3.4. Loops"},{"location":"001-python/#35-function","text":"Define a function: def my_func(x): x += 1 print('in function: ', x) return x Call a function t = 10 tt = my_func(t) print(f'out of funciton, t: {t}, tt: {tt}')","title":"3.5 Function"},{"location":"001-python/#4-basic-numpy-usage","text":"","title":"4. Basic Numpy Usage"},{"location":"001-python/#41-array-creation","text":"A numpy array is a grid of values, all of the same type, and is indexed by a tuple of integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension. We can initialize numpy arrays from nested Python lists, and access elements using square brackets: import numpy as np a = np.array([1, 2, 3]) # Create a rank 1 array print(type(a), a.dtype) print(a.shape) print(a[1]) b = np.array([[1,2,3],[4,5,6]]) # Create a rank 2 array print(b.shape) print(b[0, 0], b[0, 1], b[1, 0]) Change the type of an array: print(a.dtype) a = a.astype(float) print(a.dtype) Other array creation methods: a = np.zeros((2,2)) # Create an array of all zeros print(a) b = np.ones((1,2)) # Create an array of all ones print(b) c = np.full((2,2), 7, dtype=np.float32) # Create a constant array print(c) d = np.eye(3) # Create a 3x3 identity matrix print(d) e = np.random.random((3,3)) # Create an array filled with random values print(e)","title":"4.1. Array creation"},{"location":"001-python/#42-array-indexing","text":"Similar to Python lists, numpy arrays can be sliced. # Create a rank 1 array and reshape it to a 3x4 matrix a = np.arange(12).reshape(3, 4) b = a[:2, 1:3] print(a) print(b) # Shallow copy b[0, 0] = 888 print(a) You can mix integer indexing with slice indexing. However, integer indexing will yield an array of lower rank than the original array: row_r1 = a[1, :] # Rank 1 view of the second row of a row_r2 = a[1:2, :] # Rank 2 view of the second row of a print(row_r1, row_r1.shape) print(row_r2, row_r2.shape) You can also access array elements through lists: x = [0, 1, 2] y = [3, 1, 0] print(a[x, y]) Or through a boolean array: b = a > 4 print(b) print(a[b])","title":"4.2. Array indexing"},{"location":"001-python/#43-array-math","text":"Basic mathematical functions operate element-wise on arrays, and are available both as operator overloads and as functions in the numpy module: x = np.arange(1, 5, dtype=float).reshape(2, 2) y = np.arange(5, 9, dtype=float).reshape(2, 2) print(x) print(y) # Elementwise sum print(x + y) print(np.add(x, y)) # Elementwise difference print(x - y) print(np.subtract(x, y)) # Elementwise product print(x * y) print(np.multiply(x, y)) # Elementwise division print(x / y) print(np.divide(x, y)) # Elementwise square print(x ** 2) print(np.power(x, 2)) # Elementwise square root print(x ** 0.5) print(np.sqrt(x)) Matrix multiplication is realized by np.dot or operator @ : x = np.arange(1, 5, dtype=float).reshape(2, 2) y = np.arange(5, 9, dtype=float).reshape(2, 2) print(x) print(y) v = np.array([9, 10], dtype=float) w = np.array([11, 12], dtype=float) # Inner product print(v.dot(w)) print(np.dot(v, w)) print(v @ w) # Matrix / vector product print(x.dot(v)) print(np.dot(x, v)) print(x @ v) # Matrix / matrix product print(x.dot(y)) print(np.dot(x, y)) print(x @ y) {{ hint_warning }} Attention: np.dot() and @ behaves differently when the matrix rank is larger than 2. {{ _hint }} Numpy also provides functions for performing computations within an array: print(np.sum(x)) # Compute sum of all elements; prints \"10\" print(x.sum()) # same as above print(np.sum(x, axis=0)) # Compute sum of each column; prints \"[4 6]\" print(np.sum(x, axis=1)) # Compute sum of each row; prints \"[3 7]\" To transpose a matrix, use the T attribute of an array object: print(x.T) If you have a rank >2 matrix, you can use np.transpose to specify how to permute the axes: x = np.arange(24).reshape(2, 3, 4) print(x.transpose(1, 0, 2).shape)","title":"4.3. Array math"},{"location":"001-python/#5-using-matplotlib-for-visualization","text":"import numpy as np import matplotlib.pyplot as plt # %matplotlib qt # Compute the x and y coordinates for points on a sine curve x = np.arange(0, 3 * np.pi, 0.1) y = np.sin(x) # Plot the points using matplotlib plt.plot(x, y) plt.show() # You must call plt.show() to make graphics appear. Info Note: for jupyter notebook, you can use the command %matplotlib inline to make the graphics embedded in the editor or %matplotlib qt to make them pop out. To plot multiple lines at once, and add a title, legend, and axis labels: x = np.arange(0, 3 * np.pi, 0.1) y_sin = np.sin(x) y_cos = np.cos(x) # Plot the points using matplotlib plt.plot(x, y_sin) plt.plot(x, y_cos) plt.xlabel('x axis label') plt.ylabel('y axis label') plt.title('Sine and Cosine') plt.legend(['Sine', 'Cosine']) plt.show() You can plot different things in the same figure using the subplot function. Here is an example: # Set up a subplot grid that has height 2 and width 1, # and set the first such subplot as active. plt.subplot(2, 1, 1) # Make the first plot plt.plot(x, y_sin) plt.title('Sine') # Set the second subplot as active, and make the second plot. plt.subplot(2, 1, 2) plt.plot(x, y_cos) plt.title('Cosine') # Show the figure. plt.show()","title":"5. Using Matplotlib for Visualization"},{"location":"001-python/#6-pandas-and-scikit-learn-for-data-science","text":"In this section, we will look at a data science example using pandas as data management tool and scikit-learn (sklearn) as algorithm implementation. This section is modified from this tutorial .","title":"6. Pandas and Scikit-Learn for Data Science"},{"location":"001-python/#61-import-packages","text":"import numpy as np import pandas as pd # automatically split the data into training and test set from sklearn.model_selection import train_test_split from sklearn import preprocessing # classifiers and regressors from sklearn.ensemble import RandomForestRegressor # Construct a Pipeline from the given estimators from sklearn.pipeline import make_pipeline # Exhaustive search over specified parameter values for an estimator. from sklearn.model_selection import GridSearchCV # Training objective and evaluation metrics from sklearn.metrics import mean_squared_error, r2_score # For model persistence # you can use `from sklearn.externals import joblib` if your sklearn version is earlier than 0.23 import joblib","title":"6.1. Import packages"},{"location":"001-python/#62-load-data","text":"You can download the data by clicking the link or using wget : wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv and move the file to your current folder. Then, load the csv data into memory through pandas : data = pd.read_csv('winequality-red.csv', sep=';') Or, you can directly load the data through URL. dataset_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv' data = pd.read_csv(dataset_url, sep=';') You can also load datasets stored in other formats with pandas . A detailed document is at pandas: io .","title":"6.2. Load data"},{"location":"001-python/#63-take-a-look-of-the-loaded-data","text":"The data loaded is stored in the type of pandas.core.frame.DataFrame To give a peak of the data, we can use print(data) This will return a nice-looking preview of the elements in the DataFrame. To view the name of the features of a DataFrame, one can use print(data.keys()) To access one column, i.e., all instances of a feature, e.g., pH , one can use # These will return the same result print(data['pH']) print(data.pH) To access a row, you need the DataFrame.iloc attribute: print(data.iloc[10]) We can also easily print some summary statistics: print(data.describe())","title":"6.3. Take a look of the loaded data"},{"location":"001-python/#64-split-data","text":"First, let's separate our target (y) feature from our input (X) features and divide the dataset into training and test sets using the train_test_split function: y = data.quality X = data.drop('quality', axis=1) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=0, stratify=y ) Stratifying your sample by the target variable will ensure your training set looks similar to your test set, making your evaluation metrics more reliable.","title":"6.4. Split data"},{"location":"001-python/#65-pre-processing","text":"Standardization is the process of subtracting the means from each feature and then dividing by the feature standard deviations. It is a common requirement for machine learning tasks. Many algorithms assume that all features are centered around zero and have approximately the same variance. scaler = preprocessing.StandardScaler().fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # To prove the trainig and testing sets have (nearly) zero mean and one deviation print(X_train_scaled.mean(axis=0)) print(X_train_scaled.std(axis=0)) print(X_test_scaled.mean(axis=0)) print(X_test_scaled.std(axis=0))","title":"6.5. Pre-processing"},{"location":"001-python/#66-fit-the-model","text":"If we do not need to fine-tune the hyperparameters, we can define a random forest regression model with the default hyperparameters and fit the model using regr = RandomForestRegressor() regr.fit(X_train_scaled, y_train) To examine the performance, we use the test set to calculate the scores pred = regr.predict(X_test_scaled) print(r2_score(y_test, pred)) print(mean_squared_error(y_test, pred))","title":"6.6. Fit the model"},{"location":"001-python/#67-define-the-cross-validation-pipeline","text":"Fine-tuning hyperparameters is an important job in Machine Learning since a set of carefully chosen hyperparameters may greatly improve the performance of the model. In practice, when we set up the cross-validation pipeline, we won't even need to manually fit the data. Instead, we'll simply declare the class object, like so: pipeline = make_pipeline( preprocessing.StandardScaler(), RandomForestRegressor(n_estimators=100) ) To check the hyperparameters, we may use print pipeline.get_params() or refer to the official document . Now, let's declare the hyperparameters we want to tune through cross-validation. hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'], 'randomforestregressor__max_depth': [None, 5, 3, 1] } Then, we can set a 10-fold cross validation as simple as clf = GridSearchCV(pipeline, hyperparameters, cv=10) Finally, we can automatically fine-tune the model using clf.fit(X_train, y_train) After the model fitting, if we want to check the best hyperparameters, we can use print(clf.best_params_) Same as before, we evaluate the fitted model on test set pred = clf.predict(X_test) print(r2_score(y_test, pred)) print(mean_squared_error(y_test, pred))","title":"6.7. Define the cross-validation pipeline"},{"location":"001-python/#68-save-and-load-models","text":"After training, we may want to save the trained model for future use. For this purpose, we can use joblib.dump(clf, 'rf_regressor.pkl') When you want to load the model again, simply use this function: clf2 = joblib.load('rf_regressor.pkl') # Predict data set using loaded model clf2.predict(X_test) Info A more comprehensive example of scikit-learn can be found here .","title":"6.8. Save and load models"},{"location":"002-docker/","text":"Environment Setup Info For the purpose of the environment normalization, we provide a simple docker image for you, which contains most of the software required by this course. We also provide a few scripts to install some optional packages. The whole progress would seem as follow: Make sure you have enough resources : It requires at least 8GB of Physical RAM; 16GB or larger would be better It requires at least 15GB of hard disk storage Install a docker environment in the local machine Start Docker Service, pull images, and create an instance Just rock it! Destroy the containers and images if they are no longer needed Warning Since this docker image integrated many related services for the course, it requires at least 4GB RAM for this virtual machine. If you can not meet the minimum requirement, the system could randomly kill one or a few processes due to resource limitation, which causes a lot of strange errors which is even unable to reproduce. DO NOT TRY TO DO THAT. Instead, you can use cloud platforms such as Azure . 0. System requirements You should have enough system resources if you plan to start a container in your local OS. You need to reserve at least 4 GB RAM for Docker and some extra memory for the host machine. However, you can still start all the Hadoop-related services except Zeppelin , even if you only reserve 4GB for the virtual machine. 1. Install Docker Docker is a software providing operating-system-level virtualization, also known as containers, promoted by the company Docker, Inc. . Docker uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces, and a union-capable file system such as OverlayFS and others to allow independent \"containers\" to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines (VMs). (from Wikipedia ) Basically, you can treat docker as a lightweight virtual machine with pretty high performance. The installation instructions for different operating systems are provided in the links below. You can also check the official documentation here to get the latest news and detailed explanations. Install Docker In Linux Install Docker In macOS Install Docker In Microsoft Windows Once the docker is installed, you can start your docker services and launch your docker container using the following commands: docker # a tool to control docker 2. Run Docker image 2.1. Start the container The command to start the container used in the following tutorials is: docker run -it --privileged=true \\ --cap-add=SYS_ADMIN \\ -m 8192m -h bootcamp.local \\ --name bigbox -p 2222:22 -p 9530:9530 -p 8888:8888\\ -v /:/mnt/host \\ sunlab/bigbox:latest \\ /bin/bash It may take a while for the system to download and extract the container. In general, the syntax of docker run is docker run [options] image[:tag|@digest] [command] [args] Below explains the options used above: param -p <host-port>:<vm-port> This option is used to map the TCP port vm-port in the container to port host-port on the Docker host. Currently, the ports are reserved to: 8888 - Jupyter Notebook 9530 - Zeppelin Notebook Once you have started the Zeppelin service, this service will keep listening port 9530 in docker. You will be able to visit this service using http://127.0.0.1:9530 or http://DOCKER_HOST_IP:9530 . The remote IP depends on the Docker Service you are running, which is described above. If you are using Linux or Docker.app in macOS, you can visit \"localhost:9530\", or other ports if you changed host-port If you are using VirtualBox + macOS or Windows, you should get the Docker's IP first param -v, --volume=[host-src:]container-dest[:<options>] This option is used to mount a volume. Currently, we are using -v /:/mnt/host . In this case, we can visit the root of your file system for your host machine. If you are using macOS, /mnt/host/Users/<yourname>/ would be the $HOME of your MacBook. If you are using Windows, you can reach your C: disk from /mnt/host/c in docker. Variable host-src accepts absolute path only. param -it -i : Keep STDIN open even if not attached -t : Allocate a pseudo-tty param -h bootcamp.local Once you enter this docker environment, you can ping this docker environment itself as bootcamp.local . This variable is used in some configuration files for Hadoop ecosystems. param -m 8192m Memory limit (format: <number>[<unit>] ). The number should be a positive integer. The unit can be one of b , k , m , or g . This docker image requires at least 4G of RAM, while 8G is recommended when your physical machine has more than 8G of RAM. The local machine is not the same as the remote server. If you are launching a remote server with 8G RAM, you can set this number as 7G. Info Please refer to the official documentation for more information and the detailed explanations. 2.2. Start all necessary services When we have logged into the docker, we can first use the following script. /scripts/start-services.sh This script helps you to start the necessary services for the Hadoop ecosystems. Info In your terminal, you will generally meet two kinds of prompts. Prompt '#' indices you are root aka the administrator of this environment now > [root@bootcamp /]# whoami > root Prompt '$' indices you are an ordinary user > [yu@bootcamp /]$ whoami > yu You are in the sudo mode by default. We also assume that you are always in the sudo mode in the following discussions. Warning You probably will encounter the \"Connection Refused\" exception if you forget to start these services. If you wish to host Zeppelin, you need to install it first by using the command: /scripts/install-zeppelin.sh And start the service with the command: /scripts/start-zeppelin.sh Then, Zeppelin will listen to the port 9530 . Info Please refer to the Zeppelin tutorial for more information about the configuration and usage of Zeppelin Notebook. If you wish to host Jupyter, you can start it by using the command: /scripts/start-jupyter.sh Jupyter will listen to the port 8888 2.3. Stop all services You can stop the running services with: /scripts/stop-services.sh 2.4. Detach or Exit To detach and suspend a Docker instance from the terminal, use the command: ctrl + p, ctrl + q To exit, exit 2.5. Re-attach If you detached an instance and want to re-attach it, you need to check the CONTAINER ID or NAMES of it first. $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 011547e95ef5 sunlab/bigbox:latest \"/tini -- /bin/bash\" 6 hours ago Up 4 seconds 0.0.0.0:8888->8888/tcp, 0.0.0.0:9530->9530/tcp, 0.0.0.0:2222->22/tcp bigbox If the \"STATUS\" column is similar to \"Exited (0) 10 hours ago\", you can restart the container: $ docker start <CONTAINER ID or NAMES> And attach it with: $ docker attach <CONTAINER ID or NAMES> Every time you restart your container, you need to re-start the services (section 2.2) before any HDFS related operations. 2.6. Destroy instance If you want to permanently remove the container: $ docker rm <CONTAINER ID or NAMES> 2.7. Destroy images If you want to permanently remove any images, you need to list images first: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE sunlab/bigbox latest bfd258e00de3 16 hours ago 2.65GB And remove them by REPOSITORY or IMAGE ID using the command: $ docker rmi <REPOSITORY or IMAGE ID> 2.8. Update images $ docker pull sunlab/bigbox 2.9. Official documentations Please refer to this link for the introduction of images, containers, and storage drivers. 2.10. Optional: use docker-compose Docker Compose is a tool for defining and running multi-container Docker applications. A simple docker-compose.yml could simplify the parameters and make your life easier. Please refer to this page for further instructions. 3. Configurations and logs 3.1. System Configurations $ cat /proc/meminfo | grep Mem ## Current Memory MemTotal: 8164680 kB ## Note: This value shoud no less than 4GB MemFree: 175524 kB MemAvailable: 5113340 kB $ cat /proc/cpuinfo | grep 'model name' | head -1 ## CPU Brand model name : Intel(R) Core(TM) i7-7920HQ CPU @ 3.10GHz $ cat /proc/cpuinfo | grep 'model name' | wc -l ## CPU Count 4 $ df -h ## List Current Hard Disk Usage Filesystem Size Used Avail Use% Mounted on overlay 32G 4.6G 26G 16% / tmpfs 64M 0 64M 0% /dev ... $ ps -ef ## List Current Running Process UID PID PPID C STIME TTY TIME CMD root 1 0 0 01:38 pts/0 00:00:00 /tini -- /bin/bash root 7 1 0 01:38 pts/0 00:00:00 /bin/bash root 77 1 0 01:43 ? 00:00:00 /usr/sbin/sshd zookeep+ 136 1 0 01:43 ? 00:00:14 /usr/lib/jvm/java-openjdk/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,ROLLINGFILE -cp /usr/lib/zookeeper/bin/../build/classes:/ yarn 225 1 0 01:43 ? 00:00:13 /usr/lib/jvm/java/bin/java -Dproc_proxyserver -Xmx1000m -Dhadoop.log.dir=/var/log/hadoop-yarn -Dyarn.log.dir=/var/log/hadoop-yarn -Dhadoop.log.file=yarn-yarn-pr ... $ lsof -i:9530 ## Find the Process Listening to Some Specific Port COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME java 3165 zeppelin 189u IPv4 229945 0t0 TCP *:9530 (LISTEN) 3.2. Logs hadoop-hdfs -- /var/log/hadoop-hdfs/* hadoop-mapreduce -- /var/log/hadoop-mapreduce/* hadoop-yarn -- /var/log/hadoop-yarn/* hbase -- /var/log/hbase/* hive -- /var/log/hive/* spark -- /var/log/spark/* zookeeper -- /var/log/zookeeper/* zeppelin -- /usr/local/zeppelin/logs/*","title":"Docker"},{"location":"002-docker/#environment-setup","text":"Info For the purpose of the environment normalization, we provide a simple docker image for you, which contains most of the software required by this course. We also provide a few scripts to install some optional packages. The whole progress would seem as follow: Make sure you have enough resources : It requires at least 8GB of Physical RAM; 16GB or larger would be better It requires at least 15GB of hard disk storage Install a docker environment in the local machine Start Docker Service, pull images, and create an instance Just rock it! Destroy the containers and images if they are no longer needed Warning Since this docker image integrated many related services for the course, it requires at least 4GB RAM for this virtual machine. If you can not meet the minimum requirement, the system could randomly kill one or a few processes due to resource limitation, which causes a lot of strange errors which is even unable to reproduce. DO NOT TRY TO DO THAT. Instead, you can use cloud platforms such as Azure .","title":"Environment Setup"},{"location":"002-docker/#0-system-requirements","text":"You should have enough system resources if you plan to start a container in your local OS. You need to reserve at least 4 GB RAM for Docker and some extra memory for the host machine. However, you can still start all the Hadoop-related services except Zeppelin , even if you only reserve 4GB for the virtual machine.","title":"0. System requirements"},{"location":"002-docker/#1-install-docker","text":"Docker is a software providing operating-system-level virtualization, also known as containers, promoted by the company Docker, Inc. . Docker uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces, and a union-capable file system such as OverlayFS and others to allow independent \"containers\" to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines (VMs). (from Wikipedia ) Basically, you can treat docker as a lightweight virtual machine with pretty high performance. The installation instructions for different operating systems are provided in the links below. You can also check the official documentation here to get the latest news and detailed explanations. Install Docker In Linux Install Docker In macOS Install Docker In Microsoft Windows Once the docker is installed, you can start your docker services and launch your docker container using the following commands: docker # a tool to control docker","title":"1. Install Docker"},{"location":"002-docker/#2-run-docker-image","text":"","title":"2. Run Docker image"},{"location":"002-docker/#21-start-the-container","text":"The command to start the container used in the following tutorials is: docker run -it --privileged=true \\ --cap-add=SYS_ADMIN \\ -m 8192m -h bootcamp.local \\ --name bigbox -p 2222:22 -p 9530:9530 -p 8888:8888\\ -v /:/mnt/host \\ sunlab/bigbox:latest \\ /bin/bash It may take a while for the system to download and extract the container. In general, the syntax of docker run is docker run [options] image[:tag|@digest] [command] [args] Below explains the options used above:","title":"2.1. Start the container"},{"location":"002-docker/#param-p-host-portvm-port","text":"This option is used to map the TCP port vm-port in the container to port host-port on the Docker host. Currently, the ports are reserved to: 8888 - Jupyter Notebook 9530 - Zeppelin Notebook Once you have started the Zeppelin service, this service will keep listening port 9530 in docker. You will be able to visit this service using http://127.0.0.1:9530 or http://DOCKER_HOST_IP:9530 . The remote IP depends on the Docker Service you are running, which is described above. If you are using Linux or Docker.app in macOS, you can visit \"localhost:9530\", or other ports if you changed host-port If you are using VirtualBox + macOS or Windows, you should get the Docker's IP first","title":"param -p &lt;host-port&gt;:&lt;vm-port&gt;"},{"location":"002-docker/#param-v-volumehost-srccontainer-destoptions","text":"This option is used to mount a volume. Currently, we are using -v /:/mnt/host . In this case, we can visit the root of your file system for your host machine. If you are using macOS, /mnt/host/Users/<yourname>/ would be the $HOME of your MacBook. If you are using Windows, you can reach your C: disk from /mnt/host/c in docker. Variable host-src accepts absolute path only.","title":"param -v, --volume=[host-src:]container-dest[:&lt;options&gt;]"},{"location":"002-docker/#param-it","text":"-i : Keep STDIN open even if not attached -t : Allocate a pseudo-tty","title":"param -it"},{"location":"002-docker/#param-h-bootcamplocal","text":"Once you enter this docker environment, you can ping this docker environment itself as bootcamp.local . This variable is used in some configuration files for Hadoop ecosystems.","title":"param -h bootcamp.local"},{"location":"002-docker/#param-m-8192m","text":"Memory limit (format: <number>[<unit>] ). The number should be a positive integer. The unit can be one of b , k , m , or g . This docker image requires at least 4G of RAM, while 8G is recommended when your physical machine has more than 8G of RAM. The local machine is not the same as the remote server. If you are launching a remote server with 8G RAM, you can set this number as 7G. Info Please refer to the official documentation for more information and the detailed explanations.","title":"param -m 8192m"},{"location":"002-docker/#22-start-all-necessary-services","text":"When we have logged into the docker, we can first use the following script. /scripts/start-services.sh This script helps you to start the necessary services for the Hadoop ecosystems. Info In your terminal, you will generally meet two kinds of prompts. Prompt '#' indices you are root aka the administrator of this environment now > [root@bootcamp /]# whoami > root Prompt '$' indices you are an ordinary user > [yu@bootcamp /]$ whoami > yu You are in the sudo mode by default. We also assume that you are always in the sudo mode in the following discussions. Warning You probably will encounter the \"Connection Refused\" exception if you forget to start these services. If you wish to host Zeppelin, you need to install it first by using the command: /scripts/install-zeppelin.sh And start the service with the command: /scripts/start-zeppelin.sh Then, Zeppelin will listen to the port 9530 . Info Please refer to the Zeppelin tutorial for more information about the configuration and usage of Zeppelin Notebook. If you wish to host Jupyter, you can start it by using the command: /scripts/start-jupyter.sh Jupyter will listen to the port 8888","title":"2.2. Start all necessary services"},{"location":"002-docker/#23-stop-all-services","text":"You can stop the running services with: /scripts/stop-services.sh","title":"2.3. Stop all services"},{"location":"002-docker/#24-detach-or-exit","text":"To detach and suspend a Docker instance from the terminal, use the command: ctrl + p, ctrl + q To exit, exit","title":"2.4. Detach or Exit"},{"location":"002-docker/#25-re-attach","text":"If you detached an instance and want to re-attach it, you need to check the CONTAINER ID or NAMES of it first. $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 011547e95ef5 sunlab/bigbox:latest \"/tini -- /bin/bash\" 6 hours ago Up 4 seconds 0.0.0.0:8888->8888/tcp, 0.0.0.0:9530->9530/tcp, 0.0.0.0:2222->22/tcp bigbox If the \"STATUS\" column is similar to \"Exited (0) 10 hours ago\", you can restart the container: $ docker start <CONTAINER ID or NAMES> And attach it with: $ docker attach <CONTAINER ID or NAMES> Every time you restart your container, you need to re-start the services (section 2.2) before any HDFS related operations.","title":"2.5. Re-attach"},{"location":"002-docker/#26-destroy-instance","text":"If you want to permanently remove the container: $ docker rm <CONTAINER ID or NAMES>","title":"2.6. Destroy instance"},{"location":"002-docker/#27-destroy-images","text":"If you want to permanently remove any images, you need to list images first: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE sunlab/bigbox latest bfd258e00de3 16 hours ago 2.65GB And remove them by REPOSITORY or IMAGE ID using the command: $ docker rmi <REPOSITORY or IMAGE ID>","title":"2.7. Destroy images"},{"location":"002-docker/#28-update-images","text":"$ docker pull sunlab/bigbox","title":"2.8. Update images"},{"location":"002-docker/#29-official-documentations","text":"Please refer to this link for the introduction of images, containers, and storage drivers.","title":"2.9. Official documentations"},{"location":"002-docker/#210-optional-use-docker-compose","text":"Docker Compose is a tool for defining and running multi-container Docker applications. A simple docker-compose.yml could simplify the parameters and make your life easier. Please refer to this page for further instructions.","title":"2.10. Optional: use docker-compose"},{"location":"002-docker/#3-configurations-and-logs","text":"","title":"3. Configurations and logs"},{"location":"002-docker/#31-system-configurations","text":"$ cat /proc/meminfo | grep Mem ## Current Memory MemTotal: 8164680 kB ## Note: This value shoud no less than 4GB MemFree: 175524 kB MemAvailable: 5113340 kB $ cat /proc/cpuinfo | grep 'model name' | head -1 ## CPU Brand model name : Intel(R) Core(TM) i7-7920HQ CPU @ 3.10GHz $ cat /proc/cpuinfo | grep 'model name' | wc -l ## CPU Count 4 $ df -h ## List Current Hard Disk Usage Filesystem Size Used Avail Use% Mounted on overlay 32G 4.6G 26G 16% / tmpfs 64M 0 64M 0% /dev ... $ ps -ef ## List Current Running Process UID PID PPID C STIME TTY TIME CMD root 1 0 0 01:38 pts/0 00:00:00 /tini -- /bin/bash root 7 1 0 01:38 pts/0 00:00:00 /bin/bash root 77 1 0 01:43 ? 00:00:00 /usr/sbin/sshd zookeep+ 136 1 0 01:43 ? 00:00:14 /usr/lib/jvm/java-openjdk/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,ROLLINGFILE -cp /usr/lib/zookeeper/bin/../build/classes:/ yarn 225 1 0 01:43 ? 00:00:13 /usr/lib/jvm/java/bin/java -Dproc_proxyserver -Xmx1000m -Dhadoop.log.dir=/var/log/hadoop-yarn -Dyarn.log.dir=/var/log/hadoop-yarn -Dhadoop.log.file=yarn-yarn-pr ... $ lsof -i:9530 ## Find the Process Listening to Some Specific Port COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME java 3165 zeppelin 189u IPv4 229945 0t0 TCP *:9530 (LISTEN)","title":"3.1. System Configurations"},{"location":"002-docker/#32-logs","text":"hadoop-hdfs -- /var/log/hadoop-hdfs/* hadoop-mapreduce -- /var/log/hadoop-mapreduce/* hadoop-yarn -- /var/log/hadoop-yarn/* hbase -- /var/log/hbase/* hive -- /var/log/hive/* spark -- /var/log/spark/* zookeeper -- /var/log/zookeeper/* zeppelin -- /usr/local/zeppelin/logs/*","title":"3.2. Logs"},{"location":"0021-docker-windows/","text":"1. Windows Docker Desktop (recommended) Warning Before installation, please check this page for system requirements. If your system does not fulfill the prerequisite, you may see the image as follow. Please use Docker Toolbox on Windows instead. Download the image from this link and follow the installer step by step. Once you have successfully installed docker desktop, you may click the button \"Docker Desktop\". It may take a few minutes to start the service. You may keep a watch on the whale on the right button. Double click the whale button, and we can find a window to modify some of the properties. Please go to the advanced tab and click the drivers you wish to share with the docker container . Note: if your homework is located on Disk-D, you may not select Disk-C, this could make your operation system safer. Go to the advanced tab, and you can edit the maximum memory used by docker. If you can execute the command docker ps -a and docker-compose without any error message returned, your configuration is successful. 2. Docker Toolbox on Windows You may install Docker Toolbox on Windows instead. Going to the instruction page, click \" Get Docker Toolbox for Windows \", you will download an installer. You are supposed to install Docker and VirtualBox during this wizard. Click \"Docker Quickstart Terminal\", you should be able to start a bash session. Close it, click virtual box. You may find there is one virtual machine is running. Close this machine, update the maximum base memory. Click the \"Docker Quickstart Terminal\" and your docker is ready. FAQ Q: VirtualBox will not boot a 64bits VM when Hyper-V is activated You may meet message as follow: Error with pre-create check: \"This computer is running Hyper-V. VirtualBox won't boot a 64bits VM when Hyper-V is activated. Either use Hyper-V as a driver, or disable the Hyper-V hypervisor. (To skip this check, use --virtualbox-no-vtx-check) You can not run VirtualBox on a system with Hyper-V enabled. Hyper-V is a tier-1 hypervisor, which does not accept other hypervisors (from here ) It seems like Docker for Windows has already resolved this issue Try to disable Hyper-V . Caution: According to some reports, this operation may damage his/her network and had to reinstall all network adapters to get Internet back, or even getting a blue screen error. Try to use Hyper-V as your backend driver. https://docs.docker.com/machine/drivers/hyper-v/","title":"0021 docker windows"},{"location":"0021-docker-windows/#1-windows-docker-desktop-recommended","text":"Warning Before installation, please check this page for system requirements. If your system does not fulfill the prerequisite, you may see the image as follow. Please use Docker Toolbox on Windows instead. Download the image from this link and follow the installer step by step. Once you have successfully installed docker desktop, you may click the button \"Docker Desktop\". It may take a few minutes to start the service. You may keep a watch on the whale on the right button. Double click the whale button, and we can find a window to modify some of the properties. Please go to the advanced tab and click the drivers you wish to share with the docker container . Note: if your homework is located on Disk-D, you may not select Disk-C, this could make your operation system safer. Go to the advanced tab, and you can edit the maximum memory used by docker. If you can execute the command docker ps -a and docker-compose without any error message returned, your configuration is successful.","title":"1. Windows Docker Desktop (recommended)"},{"location":"0021-docker-windows/#2-docker-toolbox-on-windows","text":"You may install Docker Toolbox on Windows instead. Going to the instruction page, click \" Get Docker Toolbox for Windows \", you will download an installer. You are supposed to install Docker and VirtualBox during this wizard. Click \"Docker Quickstart Terminal\", you should be able to start a bash session. Close it, click virtual box. You may find there is one virtual machine is running. Close this machine, update the maximum base memory. Click the \"Docker Quickstart Terminal\" and your docker is ready.","title":"2. Docker Toolbox on Windows"},{"location":"0021-docker-windows/#faq","text":"Q: VirtualBox will not boot a 64bits VM when Hyper-V is activated You may meet message as follow: Error with pre-create check: \"This computer is running Hyper-V. VirtualBox won't boot a 64bits VM when Hyper-V is activated. Either use Hyper-V as a driver, or disable the Hyper-V hypervisor. (To skip this check, use --virtualbox-no-vtx-check) You can not run VirtualBox on a system with Hyper-V enabled. Hyper-V is a tier-1 hypervisor, which does not accept other hypervisors (from here ) It seems like Docker for Windows has already resolved this issue Try to disable Hyper-V . Caution: According to some reports, this operation may damage his/her network and had to reinstall all network adapters to get Internet back, or even getting a blue screen error. Try to use Hyper-V as your backend driver. https://docs.docker.com/machine/drivers/hyper-v/","title":"FAQ"},{"location":"0022-docker-linux/","text":"1. Install Docker on RHEL/CentOS/Fedora Get Docker CE for CentOS Get Docker CE for Fedora In brief, you can install Docker and start the service with the following commands: sudo yum install docker-ce -y # install docker package sudo service docker start # start docker service chkconfig docker on # start-up automatically FAQ If your SELinux and BTRFS are on working, you may meet an error message as follow: # systemctl status docker.service -l ... SELinux is not supported with the BTRFS graph driver! ... Modify /etc/sysconfig/docker as follow: # Modify these options if you want to change the way the docker daemon runs #OPTIONS='--selinux-enabled' OPTIONS='' ... Restart your docker service Storage Issue: Error message found in /var/log/upstart/docker.log [graphdriver] using prior storage driver \\\"btrfs\\\"... Just delete directory /var/lib/docker and restart the Docker service 2. Install Docker on Ubuntu/Debian Get Docker CE for Ubuntu Get Docker CE for Debian Generally, you can add the repository and execute sudo apt-get install docker-ce Both Debian Series and RHEL Series can be controlled by sudo service docker start # stop, restart, ... Once you started your service, you would find a socket file /var/run/docker.sock , and then you are able to execute your docker commands.","title":"0022 docker linux"},{"location":"0022-docker-linux/#1-install-docker-on-rhelcentosfedora","text":"Get Docker CE for CentOS Get Docker CE for Fedora In brief, you can install Docker and start the service with the following commands: sudo yum install docker-ce -y # install docker package sudo service docker start # start docker service chkconfig docker on # start-up automatically","title":"1. Install Docker on RHEL/CentOS/Fedora"},{"location":"0022-docker-linux/#faq","text":"If your SELinux and BTRFS are on working, you may meet an error message as follow: # systemctl status docker.service -l ... SELinux is not supported with the BTRFS graph driver! ... Modify /etc/sysconfig/docker as follow: # Modify these options if you want to change the way the docker daemon runs #OPTIONS='--selinux-enabled' OPTIONS='' ... Restart your docker service Storage Issue: Error message found in /var/log/upstart/docker.log [graphdriver] using prior storage driver \\\"btrfs\\\"... Just delete directory /var/lib/docker and restart the Docker service","title":"FAQ"},{"location":"0022-docker-linux/#2-install-docker-on-ubuntudebian","text":"Get Docker CE for Ubuntu Get Docker CE for Debian Generally, you can add the repository and execute sudo apt-get install docker-ce Both Debian Series and RHEL Series can be controlled by sudo service docker start # stop, restart, ... Once you started your service, you would find a socket file /var/run/docker.sock , and then you are able to execute your docker commands.","title":"2. Install Docker on Ubuntu/Debian"},{"location":"0023-docker-macos/","text":"Currently, there are at least two approaches to running Docker services on macOS. 1. Option One: Docker.app We recommend this installation approach. You can visit the official website to download Docker Desktop and installation instructions. Select and download a proper version of docker image and drag it to your \"Applications\" folder to install Docker software. After installation, you can click the Docker icon in the toolbar and set the maximum memory to 4G-8G, as recommended. Docker.app requires sudo access, and the container data are stored at $HOME/Library/Containers/com.docker.docker . 2. Option Two: Homebrew + VirtualBox + Docker However, here is an alternative solution. First of all, you should make sure you have already installed HomeBrew . Secondly, you are supposed to make sure your brew is up-to-date. brew update # update brew repository brew upgrade # update all packages for brew brew doctor # check your brew status Finally, you can install VirtualBox and Docker by using the following commands: brew install Caskroom/cask/virtualbox brew install docker-machine brew install docker To keep the Docker service active, we can use brew's service manager $ brew services start docker-machine ==> Successfully started `docker-machine` (label: homebrew.mxcl.docker-machine) Check the status: $ brew services list Name Status User Plist docker-machine started name /Users/name/Library/LaunchAgents/homebrew.mxcl.docker-machine.plist Create a default instance using the following command: docker-machine create --driver virtualbox --virtualbox-memory 8192 default Please refer to this link for detailed instructions. Each time you create a new terminal window, you need to execute the following command before you use any docker commands docker * : eval $(docker-machine env default) This command appends some environment variables to your current sessions. FAQ Q: Can not connect to Docker Error Message: $ docker ps -a Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? Please make sure you have already started your session. Q: Start docker-machine Failed, Can Not Get IP Address The default manager conflicts with vpn AnyConnect . Please disconnect your AnyConnect VPN before starting the Docker server. Whether Docker is compatible with GlobalProtect VPN has not been tested yet. Q: Invalid Active Developer Path xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun Error: Failure while executing: git config --local --replace-all homebrew.analyticsmessage true try xcode-select --install and then brew update , brew upgrade , and brew doctor again Q: Where are the data for the images and hard disks? They are in $HOME/.docker","title":"0023 docker macos"},{"location":"0023-docker-macos/#1-option-one-dockerapp","text":"We recommend this installation approach. You can visit the official website to download Docker Desktop and installation instructions. Select and download a proper version of docker image and drag it to your \"Applications\" folder to install Docker software. After installation, you can click the Docker icon in the toolbar and set the maximum memory to 4G-8G, as recommended. Docker.app requires sudo access, and the container data are stored at $HOME/Library/Containers/com.docker.docker .","title":"1. Option One:  Docker.app"},{"location":"0023-docker-macos/#2-option-two-homebrew-virtualbox-docker","text":"However, here is an alternative solution. First of all, you should make sure you have already installed HomeBrew . Secondly, you are supposed to make sure your brew is up-to-date. brew update # update brew repository brew upgrade # update all packages for brew brew doctor # check your brew status Finally, you can install VirtualBox and Docker by using the following commands: brew install Caskroom/cask/virtualbox brew install docker-machine brew install docker To keep the Docker service active, we can use brew's service manager $ brew services start docker-machine ==> Successfully started `docker-machine` (label: homebrew.mxcl.docker-machine) Check the status: $ brew services list Name Status User Plist docker-machine started name /Users/name/Library/LaunchAgents/homebrew.mxcl.docker-machine.plist Create a default instance using the following command: docker-machine create --driver virtualbox --virtualbox-memory 8192 default Please refer to this link for detailed instructions. Each time you create a new terminal window, you need to execute the following command before you use any docker commands docker * : eval $(docker-machine env default) This command appends some environment variables to your current sessions.","title":"2. Option Two: Homebrew + VirtualBox + Docker"},{"location":"0023-docker-macos/#faq","text":"Q: Can not connect to Docker Error Message: $ docker ps -a Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? Please make sure you have already started your session. Q: Start docker-machine Failed, Can Not Get IP Address The default manager conflicts with vpn AnyConnect . Please disconnect your AnyConnect VPN before starting the Docker server. Whether Docker is compatible with GlobalProtect VPN has not been tested yet. Q: Invalid Active Developer Path xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun Error: Failure while executing: git config --local --replace-all homebrew.analyticsmessage true try xcode-select --install and then brew update , brew upgrade , and brew doctor again Q: Where are the data for the images and hard disks? They are in $HOME/.docker","title":"FAQ"},{"location":"003-zeppelin/","text":"Zeppelin 1. Preparation Please prepare your docker environment and start your Zeppelin service. The instructions can be found in this section . You can use shared folders between your local OS and the virtual environment on Docker. These folders can be used to access data from your local and to save data after you exit/destroy your virtual environment. Use -v option to make a shared folder from an existing local folder and a folder in a virtual environment: -v <local_folder:vm_folder> You should use the absolute path for vm_folder , but it does not need to exist. For example, if want to use ~/Data/ in my local OS as the shared folder connected with /sample_data /` in the VM, I can start a container as follows: docker run -it --privileged=true \\ --cap-add=SYS_ADMIN \\ -m 8192m -h bootcamp1.docker \\ --name bigbox -p 2222:22 -p 9530:9530 -p 8888:8888\\ -v /path/to/Data/:/sample_data/ \\ sunlab/bigbox:latest \\ /bin/bash 2. Install Zeppelin and start service 2.1. Installation If you have not installed Zeppelin, you can install it with /scripts/install-zeppelin.sh You may encounter this error while installing Zeppelin: ERROR: cannot verify sunlab.org's certificate, issued by \u2018/C=US/O=Let's Encrypt/CN=R3\u2019: Issued certificate has expired. To connect to sunlab.org insecurely, use `--no-check-certificate'. To resolve this issue, you may need to modify the install-zeppelin.sh script using vi ./scripts/install-zeppelin.sh This will enter the vi editor in the terminal. In the editor, navigate to line 25 starting with wget ... , hit i to switch to the insert mode, and add the argument --no-check-certificate at the end. The updated line will look like wget http://sunlab.org/teaching/download/zeppelin-0.7.3-bin-netinst.tgz --no-check-certificate Once you have finished doing so, hit Esc to exit the insert mode, type :wq , and hit Enter to exit vi editor. You will be able to install zeppelin with ./scripts/install-zeppelin.sh now. 2.2. Starting service Then, you can start Zeppelin service with /scripts/start-zeppelin.sh Warning Make sure you have already started other necessary services with ./scripts/start-services.sh before installing Zeppelin. In addition, we need to create an HDFS folder for the user zeppelin as: sudo su - hdfs # switch to user 'hdfs' hdfs dfs -mkdir -p /user/zeppelin # create folder in hdfs hdfs dfs -chown zeppelin /user/zeppelin # change the folder owner exit You can check whether it has been created or not by using: hdfs dfs -ls /user/ 3. Open Zeppelin Notebook in your browser Once you have started Zeppelin service and have created the HDFS folder for Zeppelin, you can access Zeppelin Notebook using your local web browser. Open your web browser, and type in the address: <host-ip>:<port-for-zeppelin> For example, the address is 192.168.99.100:9530 if the IP address assigned to your Docker container is 192.168.99.100 , and the port number assigned to Zeppelin service is 9530 as default in our Docker image. Info You can check your docker host IP address using ifconfig (Linux/macOS) or ipconfig (Windows). The IP address is located at Ethernet adapter vEthernet->IPv4 Address . In fact, the docker host IP address is automatically mapped to url http://host.docker.internal . So, a simpler approach is directly using the url http://host.docker.internal:9530 to access Zeppelin Notebook ( reference ). Once you navigate to that IP address with the port number, you will see the front page of Zeppelin like 4. Create a new Notebook Click on 'Create new note', and give a name, click on 'Create Note': Then, you will see a new blank note: Next, click the gear icon on the top-right, interpreter binding setting will be unfolded. Default interpreters will be enough for the most of cases, but you can add/remove them at 'interpreter' menu if you want to. Click on 'Save' once you complete your configuration. 5. Basic usage You can click the gear icon at the right side of the paragraph. If you click 'Show title' you can give a title as you want for each paragraph. Try to use other commands also. 5.1. Text note Like other Notebooks, e.g., Jupyter, we can put some text in a paragraph by using md command with Markdown syntax: %md <some text using markdown syntax> Afterwards, click the play button or use key combination Shift+Enter to run the paragraph. It will show formatted Markdown text. You can also choose to show or hide editor for better visual effect. 5.2. Scala code If you bind default interpreters, you can use scala codes as well as Spark API in a paragraph directly: Again, do not forget to actually run the paragraph. 5.3. Possible Error If you encounter an error related to HDFS, please check whether you have created an HDFS user folder for 'zeppelin' as described above. 6. Load Data Into Table We can use SQL query statements for easier visualization with Zeppelin. Later, you can fully utilize Angular or D3 in Zeppelin for better or more sophisticated visualization. Let's get the \"Bank\" data from the official Zeppelin tutorial. Info You can find the tutorial at Zeppelin Tutorial/Basic Features (Spark) on the Welcome page. Next, define a case class for easy transformation into DataFrame and map the text data we downloaded into DataFrame without its header. Finally, register this DataFrame as Table to use sql query statements. 7. Visualization of Data via SQL query statement Once data is loaded into Table , you can use SQL query to visualize the data you want to see: %sql <valid SQL statement> Let's try to show a distribution of age of those who are younger than 30. As you can see, the visualization tool will be automatically loaded once you run a paragraph with an SQL statement. The default one is the result table of the query statement, but you can choose other types of visualization such as bar chart, pie chart, and line chart by just clicking the icons. Also, you can change configurations for each chart as you want 7.1. Input Form You can create an input form by using ${formName} or ${formName=defaultValue} templates. 7.2. Select Form Also, you can create a select form by using ${formName=defaultValue,option1|option2...} For more dynamic forms, please refer to zeppelin-dynamicform 8. Export/Import Notebook Once you've finished your work, you can export Notebook as a JSON file for later use. Also, you can import Notebook exported as JSON or from URL. Info You can download the JSON file for this tutorial here or see the official 'Zeppelin Tutorial' on the frontpage of Zeppelin.","title":"Zeppelin"},{"location":"003-zeppelin/#zeppelin","text":"","title":"Zeppelin"},{"location":"003-zeppelin/#1-preparation","text":"Please prepare your docker environment and start your Zeppelin service. The instructions can be found in this section . You can use shared folders between your local OS and the virtual environment on Docker. These folders can be used to access data from your local and to save data after you exit/destroy your virtual environment. Use -v option to make a shared folder from an existing local folder and a folder in a virtual environment: -v <local_folder:vm_folder> You should use the absolute path for vm_folder , but it does not need to exist. For example, if want to use ~/Data/ in my local OS as the shared folder connected with /sample_data /` in the VM, I can start a container as follows: docker run -it --privileged=true \\ --cap-add=SYS_ADMIN \\ -m 8192m -h bootcamp1.docker \\ --name bigbox -p 2222:22 -p 9530:9530 -p 8888:8888\\ -v /path/to/Data/:/sample_data/ \\ sunlab/bigbox:latest \\ /bin/bash","title":"1. Preparation"},{"location":"003-zeppelin/#2-install-zeppelin-and-start-service","text":"","title":"2. Install Zeppelin and start service"},{"location":"003-zeppelin/#21-installation","text":"If you have not installed Zeppelin, you can install it with /scripts/install-zeppelin.sh You may encounter this error while installing Zeppelin: ERROR: cannot verify sunlab.org's certificate, issued by \u2018/C=US/O=Let's Encrypt/CN=R3\u2019: Issued certificate has expired. To connect to sunlab.org insecurely, use `--no-check-certificate'. To resolve this issue, you may need to modify the install-zeppelin.sh script using vi ./scripts/install-zeppelin.sh This will enter the vi editor in the terminal. In the editor, navigate to line 25 starting with wget ... , hit i to switch to the insert mode, and add the argument --no-check-certificate at the end. The updated line will look like wget http://sunlab.org/teaching/download/zeppelin-0.7.3-bin-netinst.tgz --no-check-certificate Once you have finished doing so, hit Esc to exit the insert mode, type :wq , and hit Enter to exit vi editor. You will be able to install zeppelin with ./scripts/install-zeppelin.sh now.","title":"2.1. Installation"},{"location":"003-zeppelin/#22-starting-service","text":"Then, you can start Zeppelin service with /scripts/start-zeppelin.sh Warning Make sure you have already started other necessary services with ./scripts/start-services.sh before installing Zeppelin. In addition, we need to create an HDFS folder for the user zeppelin as: sudo su - hdfs # switch to user 'hdfs' hdfs dfs -mkdir -p /user/zeppelin # create folder in hdfs hdfs dfs -chown zeppelin /user/zeppelin # change the folder owner exit You can check whether it has been created or not by using: hdfs dfs -ls /user/","title":"2.2. Starting service"},{"location":"003-zeppelin/#3-open-zeppelin-notebook-in-your-browser","text":"Once you have started Zeppelin service and have created the HDFS folder for Zeppelin, you can access Zeppelin Notebook using your local web browser. Open your web browser, and type in the address: <host-ip>:<port-for-zeppelin> For example, the address is 192.168.99.100:9530 if the IP address assigned to your Docker container is 192.168.99.100 , and the port number assigned to Zeppelin service is 9530 as default in our Docker image. Info You can check your docker host IP address using ifconfig (Linux/macOS) or ipconfig (Windows). The IP address is located at Ethernet adapter vEthernet->IPv4 Address . In fact, the docker host IP address is automatically mapped to url http://host.docker.internal . So, a simpler approach is directly using the url http://host.docker.internal:9530 to access Zeppelin Notebook ( reference ). Once you navigate to that IP address with the port number, you will see the front page of Zeppelin like","title":"3. Open Zeppelin Notebook in your browser"},{"location":"003-zeppelin/#4-create-a-new-notebook","text":"Click on 'Create new note', and give a name, click on 'Create Note': Then, you will see a new blank note: Next, click the gear icon on the top-right, interpreter binding setting will be unfolded. Default interpreters will be enough for the most of cases, but you can add/remove them at 'interpreter' menu if you want to. Click on 'Save' once you complete your configuration.","title":"4. Create a new Notebook"},{"location":"003-zeppelin/#5-basic-usage","text":"You can click the gear icon at the right side of the paragraph. If you click 'Show title' you can give a title as you want for each paragraph. Try to use other commands also.","title":"5. Basic usage"},{"location":"003-zeppelin/#51-text-note","text":"Like other Notebooks, e.g., Jupyter, we can put some text in a paragraph by using md command with Markdown syntax: %md <some text using markdown syntax> Afterwards, click the play button or use key combination Shift+Enter to run the paragraph. It will show formatted Markdown text. You can also choose to show or hide editor for better visual effect.","title":"5.1. Text note"},{"location":"003-zeppelin/#52-scala-code","text":"If you bind default interpreters, you can use scala codes as well as Spark API in a paragraph directly: Again, do not forget to actually run the paragraph.","title":"5.2. Scala code"},{"location":"003-zeppelin/#53-possible-error","text":"If you encounter an error related to HDFS, please check whether you have created an HDFS user folder for 'zeppelin' as described above.","title":"5.3. Possible Error"},{"location":"003-zeppelin/#6-load-data-into-table","text":"We can use SQL query statements for easier visualization with Zeppelin. Later, you can fully utilize Angular or D3 in Zeppelin for better or more sophisticated visualization. Let's get the \"Bank\" data from the official Zeppelin tutorial. Info You can find the tutorial at Zeppelin Tutorial/Basic Features (Spark) on the Welcome page. Next, define a case class for easy transformation into DataFrame and map the text data we downloaded into DataFrame without its header. Finally, register this DataFrame as Table to use sql query statements.","title":"6. Load Data Into Table"},{"location":"003-zeppelin/#7-visualization-of-data-via-sql-query-statement","text":"Once data is loaded into Table , you can use SQL query to visualize the data you want to see: %sql <valid SQL statement> Let's try to show a distribution of age of those who are younger than 30. As you can see, the visualization tool will be automatically loaded once you run a paragraph with an SQL statement. The default one is the result table of the query statement, but you can choose other types of visualization such as bar chart, pie chart, and line chart by just clicking the icons. Also, you can change configurations for each chart as you want","title":"7. Visualization of Data via SQL query statement"},{"location":"003-zeppelin/#71-input-form","text":"You can create an input form by using ${formName} or ${formName=defaultValue} templates.","title":"7.1. Input Form"},{"location":"003-zeppelin/#72-select-form","text":"Also, you can create a select form by using ${formName=defaultValue,option1|option2...} For more dynamic forms, please refer to zeppelin-dynamicform","title":"7.2. Select Form"},{"location":"003-zeppelin/#8-exportimport-notebook","text":"Once you've finished your work, you can export Notebook as a JSON file for later use. Also, you can import Notebook exported as JSON or from URL. Info You can download the JSON file for this tutorial here or see the official 'Zeppelin Tutorial' on the frontpage of Zeppelin.","title":"8. Export/Import Notebook"},{"location":"004-scala/","text":"Scala Basics In this section we will briefly go through the essential knowledge about Scala. You will first learn how to work with Scala shell, then learn how to use variables, functions with examples. Finally, we give instructions about how to compile and run a standalone program using sbt . 1. Scala shell You can open a Scala shell by typing scala . Info Or, you can use sbt by typing sbt console . The second approach will help you add your project source code and dependencies into class path, so that your functions or library functions will be available for you to try in the interactive shell. The third approach is creating a new note in the Zeppelin notebook and choosing the default spark as the interpreter. In this tutorial, we will use the interactive shell or Zeppelin for simplicity. Once starting the Scala shell you will see $ scala Welcome to Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0). Type in expressions to have them evaluated. Type :help for more information. scala> You can type :quit to stop and quit the shell, but don't do that now :-) Next you will learn some Scala operations in the shell with the following materials. 2. Variables 2.1. val and var In Scala, there are two types of variable, immutable val and mutable var . Unlike some functional programming language that requires immutable variables, Scala allows existence of mutable variables but immutable is recommended as it is easier to verify the correctness of your program. 2.1.1. val Define an immutable variable as scala> val myInt = 1 + 1 myInt: Int = 2 scala> myInt = 3 where val is a keyword in scala that makes the variables immutable. If you reassign a value to myInt , error will be reported. scala> myInt = 3 <console>:8: error: reassignment to val myInt = 3 ^ scala> The synopsis of a varialbe is: scala> val i:String = \"abc\" i: String = abc val means it it is immutable variable, you can use \"var\" to define a mutable variable i is the name of this variable String is the type of this string, it can be omitted here \"abc\" is the value of this variable Warning In the interactive shell, it's possible to redefine variable with same name. In Scala source code files, it's not allowed. scala> val a = 1 a: Int = 1 scala> val a = 2 a: Int = 2 2.1.2. var Instead, variables declared with var are mutable. Ideally, we try to use val instead of var if possible as a good practice of functional programming. Info You may have concern that maybe too many immutable variables will be declared. Actually, with chained function calls, that situation is not the case for well organized code. An example of mutable variable is scala> var myString = \"Hello Big Data\" myString: String = Hello Big Data scala> myString = \"Hello Healthcare\" myString: String = Hello Healthcare 2.2. Type Scala may seem like a script language like JavaScript or Python, as variable type is not specified explicitly. In fact, Scala is a static type language and the compiler can implicitly infer the type in most cases. However, you can always specify a type as scala> val myDouble: Double = 3 myDouble: Double = 3.0 It is always encouraged to specify the type so unless the type is too obvious. Besides simple built-in variable types like Int , Double and String , you will also learn about List and Tuple in the training: scala> val myList: List[String] = List(\"this\", \"is\", \"a\", \"list\", \"of\", \"string\") myList: List[String] = List(this, is, a, list, of, string) scala> val myTuple: (Double, Double) = (1.0, 2.0) myTuple: (Double, Double) = (1.0,2.0) Here the List[String] is syntax of generics in Scala, which is same as C# . In the above example, List[String] means a List of String . Similarly, (Double, Double) means a two-field tuple type and both the 1st element and the 2nd element should be of type Double . 3. Functions You can define a function and invoke the function like scala> def foo(v0:Int, v1:Int):Int = { | println(v0 max v1) | v0 + v1 | } foo: (v0: Int, v1: Int)Int scala> foo(1, 2) 2 res0: Int = 3 Another example is scala> def triple(x: Int): Int = { x*3 } triple: (x: Int)Int scala> triple(2) res0: Int = 6 Where x: Int is a parameter and its type, and the second Int is the function ` return type . There's no explicit return statement, but the result of the last expression ( v0 + v1 and x * 3 ) will be returned (similar to some other programming languages like Ruby ). In this example, as there is only one expression and the return type can be inferred by the compiler, you may define the function as def triple(x: Int) = x*3 Scala is object-oriented (OO), function calls on a class method are straightforward like most OO languages (e.g. Python, Java, C#) scala> myString = \"Hello Healthcare\" myString: String = Hello Healthcare scala> myString.lastIndexOf(\"Healthcare\") res1: Int = 6 If the function does not have parameters, you can even call it without parenthesis scala> val myInt = 2 scala> myInt.toString res2: String = 2 You can also define an anonymous function and pass it to a variable like a lambda expression in some other languages such as Python: scala> val increaseOne = (x: Int) => x + 1 increaseOne: Int => Int = <function1> scala> increaseOne(3) res3: Int = 4 Anonymous function is very useful as it can be passed as a parameter to a function call scala> myList.foreach{item: String => println(item)} this is a list of string where item: String => println(item) is an anonymous function. Info Please refer to this page and this page for more information about curly braces and parentheses. This function call can be further simplified to scala> myList.foreach(println(_)) scala> myList.foreach(println) where _ represents the first parameter of the anonymous function with body println(_) . Additional _ can be specified to represent more than one parameter. For example, we can calculate the total payment that a patient made by scala> val payments = List(1, 2, 3, 4, 5, 6) payments: List[Int] = List(1, 2, 3, 4, 5, 6) scala> payments.reduce(_ + _) res0: Int = 21 In the above example, reduce will aggregate List[V] into V and we defined the aggregator as _ + _ to sum them up. Of course, you can write that more explicitly like scala> payments.reduce((a, b) => a+b) res1: Int = 21 Here reduce is a construct from functional programming. It can be illustrated with the figure below where a function f is applied to one element at a time and the result together with next element will be parameters of the next function call until the end of the list. It's important to remember that for reduce operation, the input is List[V] and the output is V . You can learn more about such operations from Wikipedia . In contrast to reduce , you can of course write code using for loop, which is verbose and very rare in Scala, scala> var totalPayment = 0 totalPayment: Int = 0 scala> for (payment <- payments) { totalPayment += payment } scala> totalPayment res2: Int = 21 3.1. Code Blocks We can create a code block anywhere, and the last line is the result of this block. For example, def foo(i:Int) = { println(s\"value: $i\") i * 2 } val newList = List[Int](1, 2, 3).map(i => foo(i)) We can use the follow lines instead: val newList = List[Int](1, 2, 3).map(i => { println(s\"value: $i\") i * 2 }) A better practice here is: val newList = List[Int](1, 2, 3).map{i => println(s\"value: $i\") i * 2 } 4. Class Declaration of a class in Scala is as simple as scala> class Foo(a:String, b:Int) { | def length = a.length | } defined class Foo scala> val foo:Foo = new Foo(\"Hello, World!\", 3) foo: Foo = Foo@6438a396 scala> println(foo.length) 13 Another example is scala> class Patient(val name: String, val id: Int) defined class Patient scala> val patient = new Patient(\"Bob\", 1) patient: Patient = Patient@755f5e80 scala> patient.name res13: String = Bob Here we see the succinct syntax of Scala again. class Patient(val name: String, val id: Int) not only defines constructor of Patient but also defines two member variables ( name and id ). A special kind of class that we will use a lot is the Case Class . For example, Case Class can be declared as scala> case class Foo(a:String, b:Int) defined class Foo scala> val foo:Foo = Foo(a = \"Hello, World!\", b = 3) foo: Foo = Foo(Hello, World!,3) scala> println(foo.a) Hello, World! Info Please refer to this page for the differences between case class and class . 4.1. Object Functions/variables in Object is similar to the static function and variable in Java. scala> object Foo { | def greeting() { | println(\"Greeting from Foo\") | } | } defined object Foo scala> Foo.greeting() Greeting from Foo What is ought to be highlighted is the usage of \"apply\". SomeObject.apply(v:Int) equals SomeObject(v:Int) scala> case class Foo(a:String, b:Int) | object Bar { | def apply(a:String): Foo = | Foo(a, a.length) | } defined class Foo defined object Bar scala> val foo = Bar(\"Hello, World!\") foo: Foo = Foo(Hello, World!,13) Warning You may get a warning if you execute the above code individually. warning: previously defined object Foo is not a companion to class Foo. Companions must be defined together; you may wish to use :paste mode for this. If you are using a terminal, a better practice is scala> :paste // Entering paste mode (ctrl-D to finish) case class Foo(a:String, b:Int) object Bar { def apply(a:String): Foo = Foo(a, a.length) } // Exiting paste mode, now interpreting. defined class Foo defined object Bar scala> val foo = Bar(\"Hello, World!\") foo: Foo = Foo(Hello, World!,13) 5. Pattern Matching You may know the switch..case in other languages. Scala provides a more flexible and powerful technique, Pattern Matching . The below example shows one can match by-value and by-type in one match. val payment:Any = 21 payment match { case p: String => println(\"payment is a String\") case p: Int if p > 30 => println(\"payment > 30\") case p: Int if p == 0 => println(\"zero payment\") case _ => println(\"otherwise\") } It's very convenient to use case class in pattern matching scala> case class Patient(val name: String, val id: Int) scala> val p = new Patient(\"Abc\", 1) p: Patient = Patient(Abc,1) scala> p match {case Patient(\"Abc\", id) => println(s\"matching id is $id\")} matching id is 1 Here we not only matched p as Patient type, but also matched the patient name and extracted one member field from the Patient class instance. Info How to add default case to above match example? Answer : p match { case Patient(\"Abc\", id) => println(s\"matching id is $id\") case _ => println(\"not matched\") } 6. Case Study of some Common Types 6.1. Option, Some, None We can use null in Scala as a null pointer, but it is not recommended. We are supposed to use Option[SomeType] to indicate this variable is optional. We can assume every variable without Option is not a null pointer if we are not calling Java code. There are two methods to check whether an Option variable is null (undefined) or not. The first one is Some : val oi = Option(1) val i = oi match { case Some(ri) => ri case None => -1 } println(i) Another set of examples are: val myMap: Map[String, String] = Map(\"key1\" -> \"value\") val value1: Option[String] = myMap.get(\"key1\") val value2: Option[String] = myMap.get(\"key2\") val i = myMap.get(\"key1\") match { case Some(ri) => ri case None => \"None\" } println(i) val i = myMap.get(\"key2\") match { case Some(ri) => ri case None => \"None\" } println(i) The second method is the function isDefined / isEmpty . val oi = Option(1) if(oi.isDefined) { println(s\"oi: ${oi.get}\") } else { println(\"oi is empty\") } Warning Option(null) returns None , but Some(null) is Some(null) which is not equals None . match is a useful reserved word, we can use it in various situations. The first is the \"switch--case\" scenario. true match { case true => println(\"true\") case false => println(\"false\") } The second usage is filtering by partial input values. This example only retrieves the value of B.a.j . _ is used as a placeholder and should never be matched. 6.2. Common methods in List, Array, Set, and so on In Scala, we always transfer the List (Array, Set, Map, etc.) from one status to another. function toList , toArray , toSet convert each other. function par Parallelize List, Array, and Map, the result of Seq[Int]().par is ParSeq[Int] , you will able to process each element in parallel when you are using foreach, map, etc., and unable to call sort before you are using toList . function distinct Removes duplicate elements function foreach Process each element and return nothing List[Int](1,2,3).foreach{ i => println(i) } It prints 1, 2, 3 in order. List[Int](1,2,3).par.foreach{ i => println(i) } Also prints 1, 2, 3, but the order is not guaranteed. function map Process each element and construct a List using the return value List[Int](1,2,3).map{ i => i + 1 } It will return List[Int](2,3,4) . The result of List[A]().map(some-oper-return-type-B) is List[B] , while the result of Array[A]().map map is Array[B] . function flatten The flatten method takes a list of lists and flattens it out to a single list: scala> List[List[Int]](List(1,2),List(3,4)).flatten res1: List[Int] = List(1, 2, 3, 4) scala> List[Option[Integer]](Some(1),Some[Integer](null),Some(2),None,Some(3)).flatten res2: List[Integer] = List(1, null, 2, 3) function flatMap The flatMap is similar to map , but it takes a function returning a list of elements as its right operand. It applies the function to each list element and returns the concatenation of all function results. The result equals to map + flatten function collect The iterator that is obtained from applying the partial function to every element in it for which it is defined and collecting the results. scala> List(1,2,3.4,\"str\") collect { | case i:Int => (i * 2).toString | case f:Double => f.toString | } res0: List[String] = List(2, 4, 3.4) The function matches elements in Int and Double, processes them, and returns the value, but ignores string elements. function filter Filter this list scala> List(1,2,3).filter(_ % 2 == 0) res1: List[Int] = List(2) function filterNot Similar to filter scala> List(1,2,3).filterNot(_ % 2 == 0) res2: List[Int] = List(1, 3) function forall Return true if All elements are return true by the partial function. It will immediately return once one element returns false, and ignore the rest elements. scala> List(2,1,0,-1).forall{ i => | val res = i > 0 | println(s\"$i > 0? $res\") | res | } 2 > 0? true 1 > 0? true 0 > 0? false res0: Boolean = false function exists Return true if there are at least One element returns true. scala> List(2,1,0,-1).exists{ i => | val res = i <= 0 | println(s\"$i <= 0? $res\") | res | } 2 <= 0? false 1 <= 0? false 0 <= 0? true res2: Boolean = true function find Return the first element returns true by the partial function. Return None if no elemet is found. scala> List(2,1,0,-1).find{ i => | val res = i <= 0 | println(s\"$i <= 0? $res\") | res | } 2 <= 0? false 1 <= 0? false 0 <= 0? true res3: Option[Int] = Some(0) function sortWith sort the elements scala> List(1,3,2).sortWith((leftOne,rightOne) => leftOne > rightOne) res5: List[Int] = List(3, 2, 1) function zipWithIndex zip the elements with appended indices List(\"a\",\"b\").zipWithIndex.foreach{ kv:(String,Int) => println(s\"k:${kv._1}, v:${kv._2}\")} It will rebuild a List with indices. k:a, v:0 k:b, v:1 keyword for Scala's keyword for can be used in various situations. Basically, for{ i <- List(1,2,3) } yield (i,i+1) It equals: List(1,2,3).map(i => (i, i+1)) Besides, for{ i <- List(1,2,3) j <- List(4,5,6) } yield (i,j) We will get the cartesian product of List(1,2,3) and List(4,5,6) : List((1,4), (1,5), (1,6), (2,4), (2,5), (2,6), (3,4), (3,5), (3,6)) We can add a filter in the condition: for{ i <- List(1,2,3) if i != 1 j <- List(4,5,6) if i * j % 2 == 1 } yield (i,j) the result is : List((3,5)) Another usage of for is as follows: Let's define variables as follows: val a = Some(1) val b = Some(2) val c = Some(3) We can execute like this: for { i <- a j <- b k <- c r <- { println(s\"i:$i, j:$j, k:$k\") Some(i * j * k) } } yield r The response is: i:1, j:2, k:3 res9: Option[Int] = Some(6) Let's define b as None: scala> val b:Option[Int] = None b: Option[Int] = None scala> for { | i <- a | j <- b | k <- c | r <- { | println(s\"i: $i, j:$j, k:$k\") | Some(i * j * k) | } | } yield r res14: Option[Int] = None keyword while Similar to while in java var i = 0 while ({ i = i + 1 i < 1000 }){ // body of while println(s\"i: $i\") } keyword to , until \u2014 (1 to 10) will generate a Seq, with the content of (1,2,3,4\u202610), (0 until 10) will generate a sequence from 0 to 9. With some test, (0 until 1000).map(xxx) appears to be slower than var i=0; while( i < 1000) { i += 1; sth. else} , but if the body of map is pretty heavy, this cost can be ignored. function headOption Get the head of one list, return None if this list is empty function head Get the head of one list, throw an exception if this list is empty function take Get first at most N elements. (from left to right) scala> List(1,2,3).take(2) res0: List[Int] = List(1, 2) scala> List(1,2).take(3) res1: List[Int] = List(1, 2) function drop Drop first at most N elements. scala> List(1,2,3).drop(2) res2: List[Int] = List(3) scala> List(1,2).drop(3) res3: List[Int] = List() function dropRight drop elements from right to left. function slice Return list in [start-offset, end-offset) scala> List(1,2,3).slice(1,2) res7: List[Int] = List(2) scala> List(1,2,3).slice(2,2) res8: List[Int] = List() val offset = 1 val size = 3 List(1,2,3,4,5).slice(offset, size + offset) If the end-offset is greater than the length of this list, it will not throw an exception. function splitAt Split this list into two from offset i scala> List(1,2,3).splitAt(1) res10: (List[Int], List[Int]) = (List(1),List(2, 3)) function groupBy Partitions a list into a map of collections according to a discriminator function scala> List(1,2,3).groupBy(i => if(i % 2 == 0) \"even\" else \"odd\" ) res11: scala.collection.immutable.Map[String,List[Int]] = Map(odd -> List(1, 3), even -> List(2)) function partition Splits a list into a pair of collections; one with elements that satisfy the predicate, the other with elements that do not, giving the pair of collections (xs filter p, xs.filterNot p). scala> List(1,2,3).partition(_ % 2 == 0) res12: (List[Int], List[Int]) = (List(2),List(1, 3)) function grouped The grouped method chunks its elements into increments. scala> List(1,2,3,4,5).grouped(2) res13: Iterator[List[Int]] = Iterator(List(1, 2), List(3, 4), List(5)) You can visit this webpage for more information. We also highly recommended to read the book Programming in Scala for more detail instruction. 7. Standalone Program Working with large real-world applications, you usually need to compile and package your source code with some tools. Here we show how to compile and run a simple program with sbt . Run the sample code in 'hello-bigdata' folder % cd /bigdata-bootcamp/sample/hello-bigdata % sbt run Attempting to fetch sbt ######################################################################### 100.0% Launching sbt from sbt-launch-0.13.8.jar [info] ..... [info] Done updating. [info] Compiling 1 Scala source to ./hello-bigdata/target/scala-2.10/classes... [info] Running Hello Hello bigdata [success] Total time: 2 s, completed May 3, 2015 8:42:48 PM The source code file hello.scala is compiled and invoked. 8. Further Reading This is a very brief overview of important features of the Scala language. We highly recommend readers to checkout references below to get a better and more complete understanding of the Scala programming language. Twitter Scala School Official Scala Tutorial SBT tutorial","title":"Scala Basics"},{"location":"004-scala/#scala-basics","text":"In this section we will briefly go through the essential knowledge about Scala. You will first learn how to work with Scala shell, then learn how to use variables, functions with examples. Finally, we give instructions about how to compile and run a standalone program using sbt .","title":"Scala Basics"},{"location":"004-scala/#1-scala-shell","text":"You can open a Scala shell by typing scala . Info Or, you can use sbt by typing sbt console . The second approach will help you add your project source code and dependencies into class path, so that your functions or library functions will be available for you to try in the interactive shell. The third approach is creating a new note in the Zeppelin notebook and choosing the default spark as the interpreter. In this tutorial, we will use the interactive shell or Zeppelin for simplicity. Once starting the Scala shell you will see $ scala Welcome to Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0). Type in expressions to have them evaluated. Type :help for more information. scala> You can type :quit to stop and quit the shell, but don't do that now :-) Next you will learn some Scala operations in the shell with the following materials.","title":"1. Scala shell"},{"location":"004-scala/#2-variables","text":"","title":"2. Variables"},{"location":"004-scala/#21-val-and-var","text":"In Scala, there are two types of variable, immutable val and mutable var . Unlike some functional programming language that requires immutable variables, Scala allows existence of mutable variables but immutable is recommended as it is easier to verify the correctness of your program.","title":"2.1. val and var"},{"location":"004-scala/#211-val","text":"Define an immutable variable as scala> val myInt = 1 + 1 myInt: Int = 2 scala> myInt = 3 where val is a keyword in scala that makes the variables immutable. If you reassign a value to myInt , error will be reported. scala> myInt = 3 <console>:8: error: reassignment to val myInt = 3 ^ scala> The synopsis of a varialbe is: scala> val i:String = \"abc\" i: String = abc val means it it is immutable variable, you can use \"var\" to define a mutable variable i is the name of this variable String is the type of this string, it can be omitted here \"abc\" is the value of this variable Warning In the interactive shell, it's possible to redefine variable with same name. In Scala source code files, it's not allowed. scala> val a = 1 a: Int = 1 scala> val a = 2 a: Int = 2","title":"2.1.1. val"},{"location":"004-scala/#212-var","text":"Instead, variables declared with var are mutable. Ideally, we try to use val instead of var if possible as a good practice of functional programming. Info You may have concern that maybe too many immutable variables will be declared. Actually, with chained function calls, that situation is not the case for well organized code. An example of mutable variable is scala> var myString = \"Hello Big Data\" myString: String = Hello Big Data scala> myString = \"Hello Healthcare\" myString: String = Hello Healthcare","title":"2.1.2. var"},{"location":"004-scala/#22-type","text":"Scala may seem like a script language like JavaScript or Python, as variable type is not specified explicitly. In fact, Scala is a static type language and the compiler can implicitly infer the type in most cases. However, you can always specify a type as scala> val myDouble: Double = 3 myDouble: Double = 3.0 It is always encouraged to specify the type so unless the type is too obvious. Besides simple built-in variable types like Int , Double and String , you will also learn about List and Tuple in the training: scala> val myList: List[String] = List(\"this\", \"is\", \"a\", \"list\", \"of\", \"string\") myList: List[String] = List(this, is, a, list, of, string) scala> val myTuple: (Double, Double) = (1.0, 2.0) myTuple: (Double, Double) = (1.0,2.0) Here the List[String] is syntax of generics in Scala, which is same as C# . In the above example, List[String] means a List of String . Similarly, (Double, Double) means a two-field tuple type and both the 1st element and the 2nd element should be of type Double .","title":"2.2. Type"},{"location":"004-scala/#3-functions","text":"You can define a function and invoke the function like scala> def foo(v0:Int, v1:Int):Int = { | println(v0 max v1) | v0 + v1 | } foo: (v0: Int, v1: Int)Int scala> foo(1, 2) 2 res0: Int = 3 Another example is scala> def triple(x: Int): Int = { x*3 } triple: (x: Int)Int scala> triple(2) res0: Int = 6 Where x: Int is a parameter and its type, and the second Int is the function ` return type . There's no explicit return statement, but the result of the last expression ( v0 + v1 and x * 3 ) will be returned (similar to some other programming languages like Ruby ). In this example, as there is only one expression and the return type can be inferred by the compiler, you may define the function as def triple(x: Int) = x*3 Scala is object-oriented (OO), function calls on a class method are straightforward like most OO languages (e.g. Python, Java, C#) scala> myString = \"Hello Healthcare\" myString: String = Hello Healthcare scala> myString.lastIndexOf(\"Healthcare\") res1: Int = 6 If the function does not have parameters, you can even call it without parenthesis scala> val myInt = 2 scala> myInt.toString res2: String = 2 You can also define an anonymous function and pass it to a variable like a lambda expression in some other languages such as Python: scala> val increaseOne = (x: Int) => x + 1 increaseOne: Int => Int = <function1> scala> increaseOne(3) res3: Int = 4 Anonymous function is very useful as it can be passed as a parameter to a function call scala> myList.foreach{item: String => println(item)} this is a list of string where item: String => println(item) is an anonymous function. Info Please refer to this page and this page for more information about curly braces and parentheses. This function call can be further simplified to scala> myList.foreach(println(_)) scala> myList.foreach(println) where _ represents the first parameter of the anonymous function with body println(_) . Additional _ can be specified to represent more than one parameter. For example, we can calculate the total payment that a patient made by scala> val payments = List(1, 2, 3, 4, 5, 6) payments: List[Int] = List(1, 2, 3, 4, 5, 6) scala> payments.reduce(_ + _) res0: Int = 21 In the above example, reduce will aggregate List[V] into V and we defined the aggregator as _ + _ to sum them up. Of course, you can write that more explicitly like scala> payments.reduce((a, b) => a+b) res1: Int = 21 Here reduce is a construct from functional programming. It can be illustrated with the figure below where a function f is applied to one element at a time and the result together with next element will be parameters of the next function call until the end of the list. It's important to remember that for reduce operation, the input is List[V] and the output is V . You can learn more about such operations from Wikipedia . In contrast to reduce , you can of course write code using for loop, which is verbose and very rare in Scala, scala> var totalPayment = 0 totalPayment: Int = 0 scala> for (payment <- payments) { totalPayment += payment } scala> totalPayment res2: Int = 21","title":"3. Functions"},{"location":"004-scala/#31-code-blocks","text":"We can create a code block anywhere, and the last line is the result of this block. For example, def foo(i:Int) = { println(s\"value: $i\") i * 2 } val newList = List[Int](1, 2, 3).map(i => foo(i)) We can use the follow lines instead: val newList = List[Int](1, 2, 3).map(i => { println(s\"value: $i\") i * 2 }) A better practice here is: val newList = List[Int](1, 2, 3).map{i => println(s\"value: $i\") i * 2 }","title":"3.1. Code Blocks"},{"location":"004-scala/#4-class","text":"Declaration of a class in Scala is as simple as scala> class Foo(a:String, b:Int) { | def length = a.length | } defined class Foo scala> val foo:Foo = new Foo(\"Hello, World!\", 3) foo: Foo = Foo@6438a396 scala> println(foo.length) 13 Another example is scala> class Patient(val name: String, val id: Int) defined class Patient scala> val patient = new Patient(\"Bob\", 1) patient: Patient = Patient@755f5e80 scala> patient.name res13: String = Bob Here we see the succinct syntax of Scala again. class Patient(val name: String, val id: Int) not only defines constructor of Patient but also defines two member variables ( name and id ). A special kind of class that we will use a lot is the Case Class . For example, Case Class can be declared as scala> case class Foo(a:String, b:Int) defined class Foo scala> val foo:Foo = Foo(a = \"Hello, World!\", b = 3) foo: Foo = Foo(Hello, World!,3) scala> println(foo.a) Hello, World! Info Please refer to this page for the differences between case class and class .","title":"4. Class"},{"location":"004-scala/#41-object","text":"Functions/variables in Object is similar to the static function and variable in Java. scala> object Foo { | def greeting() { | println(\"Greeting from Foo\") | } | } defined object Foo scala> Foo.greeting() Greeting from Foo What is ought to be highlighted is the usage of \"apply\". SomeObject.apply(v:Int) equals SomeObject(v:Int) scala> case class Foo(a:String, b:Int) | object Bar { | def apply(a:String): Foo = | Foo(a, a.length) | } defined class Foo defined object Bar scala> val foo = Bar(\"Hello, World!\") foo: Foo = Foo(Hello, World!,13) Warning You may get a warning if you execute the above code individually. warning: previously defined object Foo is not a companion to class Foo. Companions must be defined together; you may wish to use :paste mode for this. If you are using a terminal, a better practice is scala> :paste // Entering paste mode (ctrl-D to finish) case class Foo(a:String, b:Int) object Bar { def apply(a:String): Foo = Foo(a, a.length) } // Exiting paste mode, now interpreting. defined class Foo defined object Bar scala> val foo = Bar(\"Hello, World!\") foo: Foo = Foo(Hello, World!,13)","title":"4.1. Object"},{"location":"004-scala/#5-pattern-matching","text":"You may know the switch..case in other languages. Scala provides a more flexible and powerful technique, Pattern Matching . The below example shows one can match by-value and by-type in one match. val payment:Any = 21 payment match { case p: String => println(\"payment is a String\") case p: Int if p > 30 => println(\"payment > 30\") case p: Int if p == 0 => println(\"zero payment\") case _ => println(\"otherwise\") } It's very convenient to use case class in pattern matching scala> case class Patient(val name: String, val id: Int) scala> val p = new Patient(\"Abc\", 1) p: Patient = Patient(Abc,1) scala> p match {case Patient(\"Abc\", id) => println(s\"matching id is $id\")} matching id is 1 Here we not only matched p as Patient type, but also matched the patient name and extracted one member field from the Patient class instance. Info How to add default case to above match example? Answer : p match { case Patient(\"Abc\", id) => println(s\"matching id is $id\") case _ => println(\"not matched\") }","title":"5. Pattern Matching"},{"location":"004-scala/#6-case-study-of-some-common-types","text":"","title":"6. Case Study of some Common Types"},{"location":"004-scala/#61-option-some-none","text":"We can use null in Scala as a null pointer, but it is not recommended. We are supposed to use Option[SomeType] to indicate this variable is optional. We can assume every variable without Option is not a null pointer if we are not calling Java code. There are two methods to check whether an Option variable is null (undefined) or not. The first one is Some : val oi = Option(1) val i = oi match { case Some(ri) => ri case None => -1 } println(i) Another set of examples are: val myMap: Map[String, String] = Map(\"key1\" -> \"value\") val value1: Option[String] = myMap.get(\"key1\") val value2: Option[String] = myMap.get(\"key2\") val i = myMap.get(\"key1\") match { case Some(ri) => ri case None => \"None\" } println(i) val i = myMap.get(\"key2\") match { case Some(ri) => ri case None => \"None\" } println(i) The second method is the function isDefined / isEmpty . val oi = Option(1) if(oi.isDefined) { println(s\"oi: ${oi.get}\") } else { println(\"oi is empty\") } Warning Option(null) returns None , but Some(null) is Some(null) which is not equals None . match is a useful reserved word, we can use it in various situations. The first is the \"switch--case\" scenario. true match { case true => println(\"true\") case false => println(\"false\") } The second usage is filtering by partial input values. This example only retrieves the value of B.a.j . _ is used as a placeholder and should never be matched.","title":"6.1. Option, Some, None"},{"location":"004-scala/#62-common-methods-in-list-array-set-and-so-on","text":"In Scala, we always transfer the List (Array, Set, Map, etc.) from one status to another.","title":"6.2. Common methods in List, Array, Set, and so on"},{"location":"004-scala/#function-tolist-toarray-toset","text":"convert each other.","title":"function toList, toArray, toSet"},{"location":"004-scala/#function-par","text":"Parallelize List, Array, and Map, the result of Seq[Int]().par is ParSeq[Int] , you will able to process each element in parallel when you are using foreach, map, etc., and unable to call sort before you are using toList .","title":"function par"},{"location":"004-scala/#function-distinct","text":"Removes duplicate elements","title":"function distinct"},{"location":"004-scala/#function-foreach","text":"Process each element and return nothing List[Int](1,2,3).foreach{ i => println(i) } It prints 1, 2, 3 in order. List[Int](1,2,3).par.foreach{ i => println(i) } Also prints 1, 2, 3, but the order is not guaranteed.","title":"function foreach"},{"location":"004-scala/#function-map","text":"Process each element and construct a List using the return value List[Int](1,2,3).map{ i => i + 1 } It will return List[Int](2,3,4) . The result of List[A]().map(some-oper-return-type-B) is List[B] , while the result of Array[A]().map map is Array[B] .","title":"function map"},{"location":"004-scala/#function-flatten","text":"The flatten method takes a list of lists and flattens it out to a single list: scala> List[List[Int]](List(1,2),List(3,4)).flatten res1: List[Int] = List(1, 2, 3, 4) scala> List[Option[Integer]](Some(1),Some[Integer](null),Some(2),None,Some(3)).flatten res2: List[Integer] = List(1, null, 2, 3)","title":"function flatten"},{"location":"004-scala/#function-flatmap","text":"The flatMap is similar to map , but it takes a function returning a list of elements as its right operand. It applies the function to each list element and returns the concatenation of all function results. The result equals to map + flatten","title":"function flatMap"},{"location":"004-scala/#function-collect","text":"The iterator that is obtained from applying the partial function to every element in it for which it is defined and collecting the results. scala> List(1,2,3.4,\"str\") collect { | case i:Int => (i * 2).toString | case f:Double => f.toString | } res0: List[String] = List(2, 4, 3.4) The function matches elements in Int and Double, processes them, and returns the value, but ignores string elements.","title":"function collect"},{"location":"004-scala/#function-filter","text":"Filter this list scala> List(1,2,3).filter(_ % 2 == 0) res1: List[Int] = List(2)","title":"function filter"},{"location":"004-scala/#function-filternot","text":"Similar to filter scala> List(1,2,3).filterNot(_ % 2 == 0) res2: List[Int] = List(1, 3)","title":"function filterNot"},{"location":"004-scala/#function-forall","text":"Return true if All elements are return true by the partial function. It will immediately return once one element returns false, and ignore the rest elements. scala> List(2,1,0,-1).forall{ i => | val res = i > 0 | println(s\"$i > 0? $res\") | res | } 2 > 0? true 1 > 0? true 0 > 0? false res0: Boolean = false","title":"function forall"},{"location":"004-scala/#function-exists","text":"Return true if there are at least One element returns true. scala> List(2,1,0,-1).exists{ i => | val res = i <= 0 | println(s\"$i <= 0? $res\") | res | } 2 <= 0? false 1 <= 0? false 0 <= 0? true res2: Boolean = true","title":"function exists"},{"location":"004-scala/#function-find","text":"Return the first element returns true by the partial function. Return None if no elemet is found. scala> List(2,1,0,-1).find{ i => | val res = i <= 0 | println(s\"$i <= 0? $res\") | res | } 2 <= 0? false 1 <= 0? false 0 <= 0? true res3: Option[Int] = Some(0)","title":"function find"},{"location":"004-scala/#function-sortwith","text":"sort the elements scala> List(1,3,2).sortWith((leftOne,rightOne) => leftOne > rightOne) res5: List[Int] = List(3, 2, 1)","title":"function sortWith"},{"location":"004-scala/#function-zipwithindex","text":"zip the elements with appended indices List(\"a\",\"b\").zipWithIndex.foreach{ kv:(String,Int) => println(s\"k:${kv._1}, v:${kv._2}\")} It will rebuild a List with indices. k:a, v:0 k:b, v:1","title":"function zipWithIndex"},{"location":"004-scala/#keyword-for","text":"Scala's keyword for can be used in various situations. Basically, for{ i <- List(1,2,3) } yield (i,i+1) It equals: List(1,2,3).map(i => (i, i+1)) Besides, for{ i <- List(1,2,3) j <- List(4,5,6) } yield (i,j) We will get the cartesian product of List(1,2,3) and List(4,5,6) : List((1,4), (1,5), (1,6), (2,4), (2,5), (2,6), (3,4), (3,5), (3,6)) We can add a filter in the condition: for{ i <- List(1,2,3) if i != 1 j <- List(4,5,6) if i * j % 2 == 1 } yield (i,j) the result is : List((3,5)) Another usage of for is as follows: Let's define variables as follows: val a = Some(1) val b = Some(2) val c = Some(3) We can execute like this: for { i <- a j <- b k <- c r <- { println(s\"i:$i, j:$j, k:$k\") Some(i * j * k) } } yield r The response is: i:1, j:2, k:3 res9: Option[Int] = Some(6) Let's define b as None: scala> val b:Option[Int] = None b: Option[Int] = None scala> for { | i <- a | j <- b | k <- c | r <- { | println(s\"i: $i, j:$j, k:$k\") | Some(i * j * k) | } | } yield r res14: Option[Int] = None","title":"keyword for"},{"location":"004-scala/#keyword-while","text":"Similar to while in java var i = 0 while ({ i = i + 1 i < 1000 }){ // body of while println(s\"i: $i\") }","title":"keyword while"},{"location":"004-scala/#keyword-to-until","text":"\u2014 (1 to 10) will generate a Seq, with the content of (1,2,3,4\u202610), (0 until 10) will generate a sequence from 0 to 9. With some test, (0 until 1000).map(xxx) appears to be slower than var i=0; while( i < 1000) { i += 1; sth. else} , but if the body of map is pretty heavy, this cost can be ignored.","title":"keyword to, until"},{"location":"004-scala/#function-headoption","text":"Get the head of one list, return None if this list is empty","title":"function headOption"},{"location":"004-scala/#function-head","text":"Get the head of one list, throw an exception if this list is empty","title":"function head"},{"location":"004-scala/#function-take","text":"Get first at most N elements. (from left to right) scala> List(1,2,3).take(2) res0: List[Int] = List(1, 2) scala> List(1,2).take(3) res1: List[Int] = List(1, 2)","title":"function take"},{"location":"004-scala/#function-drop","text":"Drop first at most N elements. scala> List(1,2,3).drop(2) res2: List[Int] = List(3) scala> List(1,2).drop(3) res3: List[Int] = List()","title":"function drop"},{"location":"004-scala/#function-dropright","text":"drop elements from right to left.","title":"function dropRight"},{"location":"004-scala/#function-slice","text":"Return list in [start-offset, end-offset) scala> List(1,2,3).slice(1,2) res7: List[Int] = List(2) scala> List(1,2,3).slice(2,2) res8: List[Int] = List() val offset = 1 val size = 3 List(1,2,3,4,5).slice(offset, size + offset) If the end-offset is greater than the length of this list, it will not throw an exception.","title":"function slice"},{"location":"004-scala/#function-splitat","text":"Split this list into two from offset i scala> List(1,2,3).splitAt(1) res10: (List[Int], List[Int]) = (List(1),List(2, 3))","title":"function splitAt"},{"location":"004-scala/#function-groupby","text":"Partitions a list into a map of collections according to a discriminator function scala> List(1,2,3).groupBy(i => if(i % 2 == 0) \"even\" else \"odd\" ) res11: scala.collection.immutable.Map[String,List[Int]] = Map(odd -> List(1, 3), even -> List(2))","title":"function groupBy"},{"location":"004-scala/#function-partition","text":"Splits a list into a pair of collections; one with elements that satisfy the predicate, the other with elements that do not, giving the pair of collections (xs filter p, xs.filterNot p). scala> List(1,2,3).partition(_ % 2 == 0) res12: (List[Int], List[Int]) = (List(2),List(1, 3))","title":"function partition"},{"location":"004-scala/#function-grouped","text":"The grouped method chunks its elements into increments. scala> List(1,2,3,4,5).grouped(2) res13: Iterator[List[Int]] = Iterator(List(1, 2), List(3, 4), List(5)) You can visit this webpage for more information. We also highly recommended to read the book Programming in Scala for more detail instruction.","title":"function grouped"},{"location":"004-scala/#7-standalone-program","text":"Working with large real-world applications, you usually need to compile and package your source code with some tools. Here we show how to compile and run a simple program with sbt . Run the sample code in 'hello-bigdata' folder % cd /bigdata-bootcamp/sample/hello-bigdata % sbt run Attempting to fetch sbt ######################################################################### 100.0% Launching sbt from sbt-launch-0.13.8.jar [info] ..... [info] Done updating. [info] Compiling 1 Scala source to ./hello-bigdata/target/scala-2.10/classes... [info] Running Hello Hello bigdata [success] Total time: 2 s, completed May 3, 2015 8:42:48 PM The source code file hello.scala is compiled and invoked.","title":"7. Standalone Program"},{"location":"004-scala/#8-further-reading","text":"This is a very brief overview of important features of the Scala language. We highly recommend readers to checkout references below to get a better and more complete understanding of the Scala programming language. Twitter Scala School Official Scala Tutorial SBT tutorial","title":"8. Further Reading"},{"location":"005-spark/","text":"Spark Basics 1. Spark Shell Spark can run in several modes, including YARN client/server, Standalone, Mesos and Local. For this training, we will use local mode. Specifically, you can start the Spark interactive shell by invoking the command below in the terminal to run Spark in the local mode with two threads. Then you will see > spark-shell --master \"local[2]\" --driver-memory 6G Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties ... [messages] ... Spark context available as sc. scala> Here you can set --driver-memory according to your local setting. If your setting of driver memory is larger than the VM memory, don't forget to change the VM memory setting first. In Spark, we call the main entrance of a Spark program the driver and Spark distribute computation the workers . Here in the interactive shell, the Spark shell program is the driver. In the above example, we set the memory of the driver program to 3GB as in the local mode driver and worker are together. A driver program can access Spark through a SparkContext object, which represents a connection to a computing cluster. In the interactive shell, SparkContext is already created for you as variable sc . You can input sc to see its type. scala> sc res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@27896d3b 2. RDD Resilient Distributed Dataset (RDD) is Spark's core abstraction for working with data. An RDD is simply a fault-tolerant distributed collection of elements. You can imagine RDD as a large array whose elements cannot be accessed randomly. However, you can apply the same operations to all elements in the array easily. In Spark, all the work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute results. There are two ways to create RDDs: by distributing a collection of objects (e.g., a list or set), or by referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat. 2.1. Parallelized Collections For the demo purpose, the simplest way to create an RDD is to take an existing collection (e.g. a Scala Array) in your program and pass it to SparkContext's parallelize() method. scala> val data = Array(1, 2, 3, 4, 5) data: Array[Int] = Array(1, 2, 3, 4, 5) scala> val distData = sc.parallelize(data) distData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:23 Once created, the distributed dataset ( distData ) can be operated in parallel. For example, we can add up the elements by calling distData.reduce((a, b) => a + b) . You will see more operations on RDD later on. Warning Parallelizing a collection is useful when you are learning Spark. However, this is not encouraged in production since it requires the entire dataset to be in memory of the driver's machine first. Instead, importing data from external datasets should be employed. 2.2. External Datasets A common way for creating RDDs is loading data from external storage. Below you will learn how to load data from a file system. Here, we choose to read data from HDFS, which stands for Hadoop Distributed File System. (For more details, you could refer to HDFS Basics. ) When you use HDFS for the first time, it's likely that your home directory in HDFS has not been created yet. In the environment that we provide, there's a special user hdfs who is an HDFS administrator and has the permission to create new home directories. We need to put the data case.csv into HDFS, and to do this, run the following commands: > cd /bigdata-bootcamp/data > sudo su - hdfs > hdfs dfs -mkdir -p /input > hdfs dfs -chown -R root /input > exit > hdfs dfs -put case.csv /input What you do above is that first, you switch to the hdfs user. Then, you create a directory and change ownership of the newly created folder. (Since it is a virtual environment, you don't need to worry about using root user and its permission.) Next, similar to creating local directory via linux command mkdir , you creating a folder named input in HDFS. In hdfs dfs -mkdir , hdfs is the HDFS utility program, dfs is the subcommand to handle basic HDFS operations, -mkdir means you want to create a directory and the directory name is specified as input . These commands actually create the input directory in your home directory in HDFS. Of course, you can create it to other place with absolute or relative paths. Finally, after exiting HDFS, you copy data from local file system to HDFS using -put . scala> val lines = sc.textFile(\"/input/case.csv\") lines: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at <console>:21 Here in the example, each line of the original file will become an element in the lines RDD. Info Reading data from a file system, Spark relies on the HDFS library. In the example we assume HDFS is well configured through environmental variables or configuration files so that data is ready in HDFS. 3. RDD Operations RDDs offer two types of operations: transformations and actions : Transformations are operations on RDDs that return a new RDD, such as map() and filter() . Actions are operations that return a result to the driver program or write it to storage, such as first() and count() . Spark treats transformations and actions very differently, so understanding which type of operation you are performing is very important. You can check whether a function is a transformation or an action by looking at its return type: transformations return RDDs, whereas actions return some other data types. All transformations in Spark are lazy, in that they do not compute the results right away. Instead, they just remember the operations applied to some base dataset (e.g. an Array or a file). The transformations are only computed when an action requires a result to be returned to the driver program. Therefore, the above command of reading in a file has not actually been executed yet. We can force the evaluation of RDDs by calling any actions . Let's go through some common RDD operations using the healthcare dataset. Recall that in the file case.csv , each line is a 4-field tuple (patient-id, event-id, timestamp, value) . function count In order to know how large is our raw event sequence data, we can count the number of lines in the input file using count operation, i.e. scala> lines.count() res1: Long = 14046 Clearly, count is an action . function take You may wonder what the loaded data looks like, you can take a peek at the data. The take(k) will return the first k elements in the RDD. Spark also provides collect() which brings all the elements in the RDD back to the driver program. Note that collect() should only be used when the data is small. Both take and collect are actions . scala> lines.take(5) res2: Array[String] = Array(00013D2EFD8E45D1,DIAG78820,1166,1.0, 00013D2EFD8E45D1,DIAGV4501,1166,1.0, 00013D2EFD8E45D1,heartfailure,1166,1.0, 00013D2EFD8E45D1,DIAG2720,1166,1.0, 00013D2EFD8E45D1,DIAG4019,1166,1.0) We got the first 5 records in this RDD. However, this is hard to read due to a poor format. We can make it more readable by traversing the array to print each record on its own line. scala> lines.take(5).foreach(println) 00013D2EFD8E45D1,DIAG78820,1166,1.0 00013D2EFD8E45D1,DIAGV4501,1166,1.0 00013D2EFD8E45D1,heartfailure,1166,1.0 00013D2EFD8E45D1,DIAG2720,1166,1.0 00013D2EFD8E45D1,DIAG4019,1166,1.0 Note that in above 3 code block examples, the RDD lines has been computed (i.e. read in from file) 3 times. We can prevent this by calling lines.cache() , which will cache the RDD in memory to avoid reloading. Exercise: How to print the first 5 event-id? Answer: scala> lines.take(5).map(_.split(\",\")).map(_(1)).foreach(println) function map The map operation in Spark is similar to that of Hadoop. It's a transformation that transforms each item in the RDD into a new item by applying the provided function. Notice this map will map exactly one element from source to target. For example, suppose we are only interested in knowing IDs of patients, we use map like scala> lines.map(line => line.split(\",\")(0)) It is also possible to write a more complex, multiple-lines map function. In this case, curly braces should be used in place of parentheses. For example, we can get both patient-id and event-id as a tuple at the same time. scala> lines.map{line => val s = line.split(\",\") (s(0), s(1)) } function filter As indicated by its name, filter can transform an RDD to another RDD by keeping only elements that satisfy the filtering condition. For example, we want to count the number of events collected for a particular patient to verify amount of the data from that patient. We can use a filter function: scala> lines.filter(line => line.contains(\"00013D2EFD8E45D1\")).count() res4: Long = 200 function distinct distinct is a transformation that transforms a RDD to another by eliminating duplications. We can use that to count the number of distinct patients. In order to do this, we first extract the patient ID from each line. We use the map() function as described above. In this example, we transform each line into the corresponding patient ID by extracting only the first column. We then eliminate duplicate IDs by the distinct() function. scala> lines.map(line => line.split(\",\")(0)).distinct().count() res5: Long = 100 function group Sometimes, you will need to group the input events according to patient-id to put everything about each patient together. For example, in order to extract index date for predictive modeling, you may first group input data by patient then handle each patient separately in parallel. We can see each element in RDD is a (Key, Value) pair (patient-id, iterable[event]) . > val patientIdEventPair = lines.map{line => val patientId = line.split(\",\")(0) (patientId, line) } > val groupedPatientData = patientIdEventPair.groupByKey > groupedPatientData.take(1) res1: Array[(String, Iterable[String])] = Array((0102353632C5E0D0,CompactBuffer(0102353632C5E0D0,DIAG29181,562,1.0, 0102353632C5E0D0,DIAG29212,562,1.0, 0102353632C5E0D0,DIAG34590,562,1.0, 0102353632C5E0D0,DIAG30000,562,1.0, 0102353632C5E0D0,DIAG2920,562,1.0, 0102353632C5E0D0,DIAG412,562,1.0, 0102353632C5E0D0,DIAG28800,562,1.0, 0102353632C5E0D0,DIAG30391,562,1.0, 0102353632C5E0D0,DIAGRG894,562,1.0, 0102353632C5E0D0,PAYMENT,562,6000.0, 0102353632C5E0D0,DIAG5781,570,1.0, 0102353632C5E0D0,DIAG53010,570,1.0, 0102353632C5E0D0,DIAGE8490,570,1.0, 0102353632C5E0D0,DIAG27651,570,1.0, 0102353632C5E0D0,DIAG78559,570,1.0, 0102353632C5E0D0,DIAG56210,570,1.0, 0102353632C5E0D0,DIAG5856,570,1.0, 0102353632C5E0D0,heartfailure,570,1.0, 0102353632C5E0D0,DIAG5070,570,1.0, 0102353632C5E0D0,DIAGRG346,570,1.0,... .... function reduceByKey reduceByKey transforms an RDD[(K, V)] into RDD[(K, List[V])] (like what groupByKey does) and then apply reduce function on List[V] to get final output RDD[(K, V)] . Please be careful that we intentionally denote V as return type of reduce which should be same as input type of the list element. Suppose now we want to calculate the total payment by each patient. A payment record in the dataset is in the form of (patient-id, PAYMENT, timestamp, value) . val payment_events = lines.filter(line => line.contains(\"PAYMENT\")) val payments = payment_events.map{ x => val s = x.split(\",\") (s(0), s(3).toFloat) } val paymentPerPatient = payments.reduceByKey(_+_) The payment_events RDD returned by filter contains those records associated with payment. Each item is then transformed to a key-value pair (patient-id, payment) with map . Because each patient can have multiple payments, we need to use reduceByKey to sum up the payments for each patient. Here in this example, patient-id will be served as the key, and payment will be the value to sum up for each patient. The figure below shows the process of reduceByKey in our example: function sortBy We can then find the top-3 patients with the highest payment by using sortBy first. scala> paymentPerPatient.sortBy(_._2, false).take(3).foreach(println) and output is (0085B4F55FFA358D,139880.0) (019E4729585EF3DD,108980.0) (01AC552BE839AB2B,108530.0) Again in sortBy we use the _ placeholder, so that _._2 is an anonymous function that returns the second element of a tuple, which is the total payment a patient. The second parameter of sortBy controls the order of sorting. In the example above, false means decreasing order. Exercise : How to calculate the maximum payment of each patient? Answer: scala> val maxPaymentPerPatient = payments.reduceByKey(math.max) Here, reduceByKey(math.max) is the simplified expression of reduceByKey(math.max(_,_)) or reduceByKey((a,b) => math.max(a,b)) . math.max is a function in scala that turns the larger one of two parameters. Exercise : how to count the number of records for each drug (event-id starts with \"DRUG\") Answer: scala> val drugFrequency = lines.filter(_.contains(\"DRUG\")). map{ x => val s = x.split(\",\") (s(1), 1) }.reduceByKey(_+_) Statistics Now we have the total payment information of patients, we can run some basic statistics. For RDD consists of numeric values, Spark provides some useful statistical primitives. scala> val payment_values = paymentPerPatient.map(payment => payment._2).cache() scala> payment_values.max() res6: Float = 139880.0 scala> payment_values.min() res7: Float = 3910.0 scala> payment_values.sum() res8: Double = 2842480.0 scala> payment_values.mean() res9: Double = 28424.8 scala> payment_values.stdev() res10: Double = 26337.091771112468 Set Operations RDDs support many of the set operations, such as union and intersection , even when the RDDs themselves are not properly sets. For example, we can combine the two files by the union function. Please notice that union here is not strictly identical to union operation in mathematics as Spark will not remove duplications. scala> val linesControl = sc.textFile(\"/input/control.csv\") scala> lines.union(linesControl).count() res11: Long = 31144 Info Here, a more straightforward way is to use directory name to read in multiple files of that directory into a single RDD. scala> val lines = sc.textFile(\"/input/\") Exercise : how to count the number of drugs that appear in both case.csv and control.csv? Answer: scala> val drugCase = sc.textFile(\"/input/case.csv\"). filter(_.contains(\"DRUG\")). map(_.split(\",\")(1)). distinct() scala> val drugControl = sc.textFile(\"/input/control.csv\"). filter(_.contains(\"DRUG\")). map(_.split(\",\")(1)). distinct() scala> drugCase.intersection(drugControl).count() res: Long = 396 Datasets Warning Dataset is added from Spark 1.6+ A Dataset is a new interface added from Spark 1.6 that tries to provide the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL\u2019s optimized execution engine. A Dataset has a concrete type of a Scala primitive type (Integer, Long, Boolean, etc) or a subclass of a Product - a case class. The case class is preferred for Spark because it handles the serialization code for you thus allowing Spark to shuffle data between workers. This can additional be handled implementing Externalizable which is a much more efficient mechanism to handle serialization, or by using a compact serializer like Kryos. Additionally, Spark no longer uses SparkContext directly but prefers the use of a SparkSession that encapsulates a SparkContext and a SqlContext. The SparkSession is a member of the sql package. There is a wealth of great documentation on the Spark development site. Creating datasets Datasets can be created explicitly or loaded form a source (e.g. file, stream, parquet, etc). case class Person(firstName: String, lastName:String) // wire-in spark implicits import spark.implicits._ case class Person(firstName: String, lastName: String) val ds = Seq(Person(\"Daniel\", \"Williams\")).toDS() // here you can perform operations that are deferred until an action is invoked. // creates a anonymous lambda that looks at the // firstName of the Dataset[Person] type and invokes a collect // to pull data back to the driver as an Array[Person] // the foreach then will invoke a println on each Person // instance and implicit apply the toString operation that is // held in the Product trait ds.filter(_.firstName == \"Daniel\").collect().foreach(println) Info Please refer to Spark SQL, DataFrames and Datasets Guide for more information. 4. Further Reading For the complete list of RDD operations, please see the Spark Programming Guide .","title":"Spark Basics"},{"location":"005-spark/#spark-basics","text":"","title":"Spark Basics"},{"location":"005-spark/#1-spark-shell","text":"Spark can run in several modes, including YARN client/server, Standalone, Mesos and Local. For this training, we will use local mode. Specifically, you can start the Spark interactive shell by invoking the command below in the terminal to run Spark in the local mode with two threads. Then you will see > spark-shell --master \"local[2]\" --driver-memory 6G Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties ... [messages] ... Spark context available as sc. scala> Here you can set --driver-memory according to your local setting. If your setting of driver memory is larger than the VM memory, don't forget to change the VM memory setting first. In Spark, we call the main entrance of a Spark program the driver and Spark distribute computation the workers . Here in the interactive shell, the Spark shell program is the driver. In the above example, we set the memory of the driver program to 3GB as in the local mode driver and worker are together. A driver program can access Spark through a SparkContext object, which represents a connection to a computing cluster. In the interactive shell, SparkContext is already created for you as variable sc . You can input sc to see its type. scala> sc res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@27896d3b","title":"1. Spark Shell"},{"location":"005-spark/#2-rdd","text":"Resilient Distributed Dataset (RDD) is Spark's core abstraction for working with data. An RDD is simply a fault-tolerant distributed collection of elements. You can imagine RDD as a large array whose elements cannot be accessed randomly. However, you can apply the same operations to all elements in the array easily. In Spark, all the work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute results. There are two ways to create RDDs: by distributing a collection of objects (e.g., a list or set), or by referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.","title":"2. RDD"},{"location":"005-spark/#21-parallelized-collections","text":"For the demo purpose, the simplest way to create an RDD is to take an existing collection (e.g. a Scala Array) in your program and pass it to SparkContext's parallelize() method. scala> val data = Array(1, 2, 3, 4, 5) data: Array[Int] = Array(1, 2, 3, 4, 5) scala> val distData = sc.parallelize(data) distData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:23 Once created, the distributed dataset ( distData ) can be operated in parallel. For example, we can add up the elements by calling distData.reduce((a, b) => a + b) . You will see more operations on RDD later on. Warning Parallelizing a collection is useful when you are learning Spark. However, this is not encouraged in production since it requires the entire dataset to be in memory of the driver's machine first. Instead, importing data from external datasets should be employed.","title":"2.1. Parallelized Collections"},{"location":"005-spark/#22-external-datasets","text":"A common way for creating RDDs is loading data from external storage. Below you will learn how to load data from a file system. Here, we choose to read data from HDFS, which stands for Hadoop Distributed File System. (For more details, you could refer to HDFS Basics. ) When you use HDFS for the first time, it's likely that your home directory in HDFS has not been created yet. In the environment that we provide, there's a special user hdfs who is an HDFS administrator and has the permission to create new home directories. We need to put the data case.csv into HDFS, and to do this, run the following commands: > cd /bigdata-bootcamp/data > sudo su - hdfs > hdfs dfs -mkdir -p /input > hdfs dfs -chown -R root /input > exit > hdfs dfs -put case.csv /input What you do above is that first, you switch to the hdfs user. Then, you create a directory and change ownership of the newly created folder. (Since it is a virtual environment, you don't need to worry about using root user and its permission.) Next, similar to creating local directory via linux command mkdir , you creating a folder named input in HDFS. In hdfs dfs -mkdir , hdfs is the HDFS utility program, dfs is the subcommand to handle basic HDFS operations, -mkdir means you want to create a directory and the directory name is specified as input . These commands actually create the input directory in your home directory in HDFS. Of course, you can create it to other place with absolute or relative paths. Finally, after exiting HDFS, you copy data from local file system to HDFS using -put . scala> val lines = sc.textFile(\"/input/case.csv\") lines: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at <console>:21 Here in the example, each line of the original file will become an element in the lines RDD. Info Reading data from a file system, Spark relies on the HDFS library. In the example we assume HDFS is well configured through environmental variables or configuration files so that data is ready in HDFS.","title":"2.2. External Datasets"},{"location":"005-spark/#3-rdd-operations","text":"RDDs offer two types of operations: transformations and actions : Transformations are operations on RDDs that return a new RDD, such as map() and filter() . Actions are operations that return a result to the driver program or write it to storage, such as first() and count() . Spark treats transformations and actions very differently, so understanding which type of operation you are performing is very important. You can check whether a function is a transformation or an action by looking at its return type: transformations return RDDs, whereas actions return some other data types. All transformations in Spark are lazy, in that they do not compute the results right away. Instead, they just remember the operations applied to some base dataset (e.g. an Array or a file). The transformations are only computed when an action requires a result to be returned to the driver program. Therefore, the above command of reading in a file has not actually been executed yet. We can force the evaluation of RDDs by calling any actions . Let's go through some common RDD operations using the healthcare dataset. Recall that in the file case.csv , each line is a 4-field tuple (patient-id, event-id, timestamp, value) .","title":"3. RDD Operations"},{"location":"005-spark/#function-count","text":"In order to know how large is our raw event sequence data, we can count the number of lines in the input file using count operation, i.e. scala> lines.count() res1: Long = 14046 Clearly, count is an action .","title":"function count"},{"location":"005-spark/#function-take","text":"You may wonder what the loaded data looks like, you can take a peek at the data. The take(k) will return the first k elements in the RDD. Spark also provides collect() which brings all the elements in the RDD back to the driver program. Note that collect() should only be used when the data is small. Both take and collect are actions . scala> lines.take(5) res2: Array[String] = Array(00013D2EFD8E45D1,DIAG78820,1166,1.0, 00013D2EFD8E45D1,DIAGV4501,1166,1.0, 00013D2EFD8E45D1,heartfailure,1166,1.0, 00013D2EFD8E45D1,DIAG2720,1166,1.0, 00013D2EFD8E45D1,DIAG4019,1166,1.0) We got the first 5 records in this RDD. However, this is hard to read due to a poor format. We can make it more readable by traversing the array to print each record on its own line. scala> lines.take(5).foreach(println) 00013D2EFD8E45D1,DIAG78820,1166,1.0 00013D2EFD8E45D1,DIAGV4501,1166,1.0 00013D2EFD8E45D1,heartfailure,1166,1.0 00013D2EFD8E45D1,DIAG2720,1166,1.0 00013D2EFD8E45D1,DIAG4019,1166,1.0 Note that in above 3 code block examples, the RDD lines has been computed (i.e. read in from file) 3 times. We can prevent this by calling lines.cache() , which will cache the RDD in memory to avoid reloading. Exercise: How to print the first 5 event-id? Answer: scala> lines.take(5).map(_.split(\",\")).map(_(1)).foreach(println)","title":"function take"},{"location":"005-spark/#function-map","text":"The map operation in Spark is similar to that of Hadoop. It's a transformation that transforms each item in the RDD into a new item by applying the provided function. Notice this map will map exactly one element from source to target. For example, suppose we are only interested in knowing IDs of patients, we use map like scala> lines.map(line => line.split(\",\")(0)) It is also possible to write a more complex, multiple-lines map function. In this case, curly braces should be used in place of parentheses. For example, we can get both patient-id and event-id as a tuple at the same time. scala> lines.map{line => val s = line.split(\",\") (s(0), s(1)) }","title":"function map"},{"location":"005-spark/#function-filter","text":"As indicated by its name, filter can transform an RDD to another RDD by keeping only elements that satisfy the filtering condition. For example, we want to count the number of events collected for a particular patient to verify amount of the data from that patient. We can use a filter function: scala> lines.filter(line => line.contains(\"00013D2EFD8E45D1\")).count() res4: Long = 200","title":"function filter"},{"location":"005-spark/#function-distinct","text":"distinct is a transformation that transforms a RDD to another by eliminating duplications. We can use that to count the number of distinct patients. In order to do this, we first extract the patient ID from each line. We use the map() function as described above. In this example, we transform each line into the corresponding patient ID by extracting only the first column. We then eliminate duplicate IDs by the distinct() function. scala> lines.map(line => line.split(\",\")(0)).distinct().count() res5: Long = 100","title":"function distinct"},{"location":"005-spark/#function-group","text":"Sometimes, you will need to group the input events according to patient-id to put everything about each patient together. For example, in order to extract index date for predictive modeling, you may first group input data by patient then handle each patient separately in parallel. We can see each element in RDD is a (Key, Value) pair (patient-id, iterable[event]) . > val patientIdEventPair = lines.map{line => val patientId = line.split(\",\")(0) (patientId, line) } > val groupedPatientData = patientIdEventPair.groupByKey > groupedPatientData.take(1) res1: Array[(String, Iterable[String])] = Array((0102353632C5E0D0,CompactBuffer(0102353632C5E0D0,DIAG29181,562,1.0, 0102353632C5E0D0,DIAG29212,562,1.0, 0102353632C5E0D0,DIAG34590,562,1.0, 0102353632C5E0D0,DIAG30000,562,1.0, 0102353632C5E0D0,DIAG2920,562,1.0, 0102353632C5E0D0,DIAG412,562,1.0, 0102353632C5E0D0,DIAG28800,562,1.0, 0102353632C5E0D0,DIAG30391,562,1.0, 0102353632C5E0D0,DIAGRG894,562,1.0, 0102353632C5E0D0,PAYMENT,562,6000.0, 0102353632C5E0D0,DIAG5781,570,1.0, 0102353632C5E0D0,DIAG53010,570,1.0, 0102353632C5E0D0,DIAGE8490,570,1.0, 0102353632C5E0D0,DIAG27651,570,1.0, 0102353632C5E0D0,DIAG78559,570,1.0, 0102353632C5E0D0,DIAG56210,570,1.0, 0102353632C5E0D0,DIAG5856,570,1.0, 0102353632C5E0D0,heartfailure,570,1.0, 0102353632C5E0D0,DIAG5070,570,1.0, 0102353632C5E0D0,DIAGRG346,570,1.0,... ....","title":"function group"},{"location":"005-spark/#function-reducebykey","text":"reduceByKey transforms an RDD[(K, V)] into RDD[(K, List[V])] (like what groupByKey does) and then apply reduce function on List[V] to get final output RDD[(K, V)] . Please be careful that we intentionally denote V as return type of reduce which should be same as input type of the list element. Suppose now we want to calculate the total payment by each patient. A payment record in the dataset is in the form of (patient-id, PAYMENT, timestamp, value) . val payment_events = lines.filter(line => line.contains(\"PAYMENT\")) val payments = payment_events.map{ x => val s = x.split(\",\") (s(0), s(3).toFloat) } val paymentPerPatient = payments.reduceByKey(_+_) The payment_events RDD returned by filter contains those records associated with payment. Each item is then transformed to a key-value pair (patient-id, payment) with map . Because each patient can have multiple payments, we need to use reduceByKey to sum up the payments for each patient. Here in this example, patient-id will be served as the key, and payment will be the value to sum up for each patient. The figure below shows the process of reduceByKey in our example:","title":"function reduceByKey"},{"location":"005-spark/#function-sortby","text":"We can then find the top-3 patients with the highest payment by using sortBy first. scala> paymentPerPatient.sortBy(_._2, false).take(3).foreach(println) and output is (0085B4F55FFA358D,139880.0) (019E4729585EF3DD,108980.0) (01AC552BE839AB2B,108530.0) Again in sortBy we use the _ placeholder, so that _._2 is an anonymous function that returns the second element of a tuple, which is the total payment a patient. The second parameter of sortBy controls the order of sorting. In the example above, false means decreasing order. Exercise : How to calculate the maximum payment of each patient? Answer: scala> val maxPaymentPerPatient = payments.reduceByKey(math.max) Here, reduceByKey(math.max) is the simplified expression of reduceByKey(math.max(_,_)) or reduceByKey((a,b) => math.max(a,b)) . math.max is a function in scala that turns the larger one of two parameters. Exercise : how to count the number of records for each drug (event-id starts with \"DRUG\") Answer: scala> val drugFrequency = lines.filter(_.contains(\"DRUG\")). map{ x => val s = x.split(\",\") (s(1), 1) }.reduceByKey(_+_)","title":"function sortBy"},{"location":"005-spark/#statistics","text":"Now we have the total payment information of patients, we can run some basic statistics. For RDD consists of numeric values, Spark provides some useful statistical primitives. scala> val payment_values = paymentPerPatient.map(payment => payment._2).cache() scala> payment_values.max() res6: Float = 139880.0 scala> payment_values.min() res7: Float = 3910.0 scala> payment_values.sum() res8: Double = 2842480.0 scala> payment_values.mean() res9: Double = 28424.8 scala> payment_values.stdev() res10: Double = 26337.091771112468","title":"Statistics"},{"location":"005-spark/#set-operations","text":"RDDs support many of the set operations, such as union and intersection , even when the RDDs themselves are not properly sets. For example, we can combine the two files by the union function. Please notice that union here is not strictly identical to union operation in mathematics as Spark will not remove duplications. scala> val linesControl = sc.textFile(\"/input/control.csv\") scala> lines.union(linesControl).count() res11: Long = 31144 Info Here, a more straightforward way is to use directory name to read in multiple files of that directory into a single RDD. scala> val lines = sc.textFile(\"/input/\") Exercise : how to count the number of drugs that appear in both case.csv and control.csv? Answer: scala> val drugCase = sc.textFile(\"/input/case.csv\"). filter(_.contains(\"DRUG\")). map(_.split(\",\")(1)). distinct() scala> val drugControl = sc.textFile(\"/input/control.csv\"). filter(_.contains(\"DRUG\")). map(_.split(\",\")(1)). distinct() scala> drugCase.intersection(drugControl).count() res: Long = 396","title":"Set Operations"},{"location":"005-spark/#datasets","text":"Warning Dataset is added from Spark 1.6+ A Dataset is a new interface added from Spark 1.6 that tries to provide the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL\u2019s optimized execution engine. A Dataset has a concrete type of a Scala primitive type (Integer, Long, Boolean, etc) or a subclass of a Product - a case class. The case class is preferred for Spark because it handles the serialization code for you thus allowing Spark to shuffle data between workers. This can additional be handled implementing Externalizable which is a much more efficient mechanism to handle serialization, or by using a compact serializer like Kryos. Additionally, Spark no longer uses SparkContext directly but prefers the use of a SparkSession that encapsulates a SparkContext and a SqlContext. The SparkSession is a member of the sql package. There is a wealth of great documentation on the Spark development site. Creating datasets Datasets can be created explicitly or loaded form a source (e.g. file, stream, parquet, etc). case class Person(firstName: String, lastName:String) // wire-in spark implicits import spark.implicits._ case class Person(firstName: String, lastName: String) val ds = Seq(Person(\"Daniel\", \"Williams\")).toDS() // here you can perform operations that are deferred until an action is invoked. // creates a anonymous lambda that looks at the // firstName of the Dataset[Person] type and invokes a collect // to pull data back to the driver as an Array[Person] // the foreach then will invoke a println on each Person // instance and implicit apply the toString operation that is // held in the Product trait ds.filter(_.firstName == \"Daniel\").collect().foreach(println) Info Please refer to Spark SQL, DataFrames and Datasets Guide for more information.","title":"Datasets"},{"location":"005-spark/#4-further-reading","text":"For the complete list of RDD operations, please see the Spark Programming Guide .","title":"4. Further Reading"}]}